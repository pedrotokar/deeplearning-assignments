{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa07f56",
   "metadata": {},
   "source": [
    "# **Programming Assignment 2 - Object Detection + Semantic Segmentation**\n",
    "\n",
    "#### **Professor**: Dário Oliveira  \n",
    "#### **Monitora**: Lívia Meinhardt\n",
    "\n",
    "\n",
    "O objetivo deste trabalho é construir um pipeline de visão computacional que primeiro **detecta** objetos em uma imagem e, em seguida, realiza a **segmentação semântica** em cada objeto detectado. Ou seja, uma segmentação de instância em duas etapas. \n",
    "\n",
    "Vocês irão construir e conectar dois modelos distintos:\n",
    "1.  Um **detector de objetos** (YOLO) para encontrar a localização dos objetos.\n",
    "2.  Um **segmentador semântico** (U-Net ou outro) para classificar os pixels dentro de cada objeto localizado.\n",
    "\n",
    "### **Instruções:**\n",
    "\n",
    "1. **Criação de um Dataset**:  \n",
    "Vocês usarão o dataset **[PASCAL VOC 2012](https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset)**. Este conjunto de dados é ideal porque fornece anotações tanto para **caixas delimitadoras** (*bounding boxes*), para a tarefa de detecção, quanto para **máscaras de segmentação** a nível de pixel. \n",
    "\n",
    "\n",
    "2. **Implementação e Treinamento da YOLO:**\n",
    "Sua primeira tarefa é fazer a implementação da YOLOv3, vista em aula. Treine um modelo que recebe uma imagem como entrada e retorna uma lista de predições, onde cada predição contém `(caixa_delimitadora, classe, score_de_confiança)`. Meça o desempenho do seu detector usando a métrica **Mean Average Precision (mAP)**.\n",
    "\n",
    "\n",
    "3. **Treinar o Segmentador Semântico:**\n",
    "Sua segunda tarefa é treinar um modelo **U-Net** ou outro de sua preferência para realizar a segmentação. O ponto crucial é que vocês não irão treinar a U-Net com imagens inteiras. Em vez disso, vocês a treinarão com **recortes de imagem (*patches*)** gerados a partir das caixas delimitadoras de referência (*ground-truth*) do dataset. Meça o desempenho do seu segmentador usando a métrica **Average Precision (AP).**\n",
    "\n",
    "4. **Construir o Pipeline de Inferência:**\n",
    "    Agora, conecte seus dois modelos treinados. Esta parte envolve escrever um script que executa a tarefa completa de ponta a ponta.\n",
    "\n",
    "    1.  **Detectar**: Use uma nova imagem de teste e passe-a pelo seu modelo **YOLOv3** treinado para obter uma lista de caixas delimitadoras preditas.\n",
    "    2.  **Recortar**: Para cada predição de alta confiança do YOLO, **recorte o *patch* da imagem** definido pela caixa delimitadora.\n",
    "    3.  **Segmentar**: Passe cada *patch* recortado pelo seu modelo **U-Net** treinado para obter uma máscara de segmentação para aquele objeto específico.\n",
    "    4.  **Combinar**: Crie uma imagem em branco (preta) do mesmo tamanho da imagem original. \"Costure\" cada máscara gerada de volta nesta imagem em branco, na sua localização original da caixa delimitadora.\n",
    "    5.  **Visualizar**: Sobreponha a máscara final combinada na imagem original para criar uma visualização final mostrando todos os objetos detectados e segmentados.\n",
    "\n",
    "\n",
    "5. **Compare com um método *end-to-end:***\n",
    "Por fim, faça *fine-tuning* do [**Mask R-CNN**](https://docs.pytorch.org/vision/main/models/mask_rcnn.html), um um modelo de segmentação de instância de ponta a ponta (*end-to-end*). Compare o desempenho com o seu pipeline de dois estágios e discuta as diferenças. \n",
    "\n",
    "\n",
    "### **Entrega:**\n",
    "\n",
    "Você deve enviar:\n",
    "\n",
    "1.  Um **Jupyter Notebook** contendo todo o seu código.\n",
    "2.  Os **pesos treinados** tanto do seu detector YOLOv3 quanto do seu segmentador U-Net.\n",
    "3.  Um **relatório ou apresentação** que discuta os desafios e resultados dos seus experimentos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd05a4",
   "metadata": {},
   "source": [
    "## Preparando o dataset pra detecção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define dataset root\n",
    "original_dataset_path = 'data'\n",
    "yolo_dataset_path = 'yolo_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e97baaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The directory /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/JPEGImages or /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/Annotations does not exist. Please verify the dataset path.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m annotations_dir = os.path.join(original_dataset_path, \u001b[33m'\u001b[39m\u001b[33mVOC2012_train_val\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAnnotations\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(jpeg_images_dir) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(annotations_dir):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjpeg_images_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mannotations_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist. Please verify the dataset path.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m image_filenames = os.listdir(jpeg_images_dir)\n\u001b[32m     16\u001b[39m image_ids = [os.path.splitext(filename)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m image_filenames \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: The directory /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/JPEGImages or /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/Annotations does not exist. Please verify the dataset path."
     ]
    }
   ],
   "source": [
    "yolo_dirs = [\n",
    "    os.path.join(yolo_dataset_path, 'images', 'train'),\n",
    "    os.path.join(yolo_dataset_path, 'images', 'val'),\n",
    "    os.path.join(yolo_dataset_path, 'labels', 'train'),\n",
    "    os.path.join(yolo_dataset_path, 'labels', 'val')\n",
    "]\n",
    "\n",
    "for yolo_dir in yolo_dirs:\n",
    "    os.makedirs(yolo_dir, exist_ok=True)\n",
    "\n",
    "jpeg_images_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'JPEGImages')\n",
    "annotations_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'Annotations')\n",
    "if not os.path.exists(jpeg_images_dir) or not os.path.exists(annotations_dir):\n",
    "    raise FileNotFoundError(f\"The directory {jpeg_images_dir} or {annotations_dir} does not exist. Please verify the dataset path.\")\n",
    "image_filenames = os.listdir(jpeg_images_dir)\n",
    "image_ids = [os.path.splitext(filename)[0] for filename in image_filenames if filename.endswith('.jpg')]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids)\n",
    "split_index = int(0.8 * len(image_ids)) #Spliting the dataset 80% for training, 20% for validation\n",
    "train_ids = image_ids[:split_index] #taking the first 80% pictures\n",
    "val_ids = image_ids[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "#this fucntion converts PASCAL_VOC annotations to YOLO format\n",
    "def create_yolo_annotation(xml_file_path, yolo_label_path, label_dict):\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    annotations = [] #list that will store the converted YOLO annotations.\n",
    "\n",
    "    img_width = int(root.find('size/width').text)\n",
    "    img_height = int(root.find('size/height').text)\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        if label not in label_dict:\n",
    "            continue\n",
    "        label_idx = label_dict[label]\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = float(bndbox.find('xmin').text)\n",
    "        ymin = float(bndbox.find('ymin').text)\n",
    "        xmax = float(bndbox.find('xmax').text)\n",
    "        ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "        # this is YOLOv8 annotation format: label x_center y_center width height (normalized)\n",
    "        x_center = ((xmin + xmax) / 2) / img_width\n",
    "        y_center = ((ymin + ymax) / 2) / img_height\n",
    "        width = (xmax - xmin) / img_width\n",
    "        height = (ymax - ymin) / img_height\n",
    "\n",
    "        annotations.append(f\"{label_idx} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    #annotations to the label file\n",
    "    with open(yolo_label_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "    'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9,\n",
    "    'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,\n",
    "    'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19\n",
    "}\n",
    "\n",
    "for image_set, ids in [('train', train_ids), ('val', val_ids)]:\n",
    "    for img_id in ids:\n",
    "        img_src_path = os.path.join(jpeg_images_dir, f'{img_id}.jpg')\n",
    "        label_dst_path = os.path.join(yolo_dataset_path, 'labels', image_set, f'{img_id}.txt')\n",
    "\n",
    "        # Create the YOLO annotation file\n",
    "        xml_file_path = os.path.join(annotations_dir, f'{img_id}.xml')\n",
    "        if not os.path.exists(xml_file_path):\n",
    "            print(f\"Warning: Annotation {xml_file_path} not found, skipping.\")\n",
    "            continue\n",
    "        create_yolo_annotation(xml_file_path, label_dst_path, label_dict)\n",
    "\n",
    "        # Copy the image to the new YOLO dataset structure\n",
    "        img_dst_path = os.path.join(yolo_dataset_path, 'images', image_set, f'{img_id}.jpg')\n",
    "        shutil.copy(img_src_path, img_dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ae84d",
   "metadata": {},
   "source": [
    "### Testando na YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30200929",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "nc: {len(label_dict)}\n",
    "names: {list(label_dict.keys())}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(yolo_dataset_path, 'data.yaml'), 'w') as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79713165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # Use the YOLOv8 nano model\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=os.path.join(yolo_dataset_path, 'data.yaml'),  # Path to dataset config file\n",
    "    epochs=2,  # Number of epochs\n",
    "    imgsz=640,  # Image size\n",
    "    batch=16,  # Batch size\n",
    "    name='yolov8_test'  # Experiment name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source=os.path.join(yolo_dataset_path, 'images/val/'), save=True)  # Run inference on validation set\n",
    "\n",
    "# Display predictions\n",
    "for result in results:\n",
    "    result.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5882cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf = 0\n",
    "best_result = None\n",
    "\n",
    "for result in results:    \n",
    "        if (result.boxes.conf.mean().item() > max_conf) and (result.boxes.conf.mean().item() < 0.98) and (result.boxes.conf.shape[0] > 2):\n",
    "            max_conf = result.boxes.conf.mean().item()\n",
    "            best_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "img = best_result.orig_img\n",
    "boxes = best_result.boxes.xyxy.cpu().numpy()  # xyxy format\n",
    "classe = best_result.boxes.cls\n",
    "confianca = best_result.boxes.conf\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = box[:4]\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    # Add class and confidence\n",
    "    class_idx = int(classe[i])\n",
    "    class_name = best_result.names[class_idx] if hasattr(best_result, 'names') else str(class_idx)\n",
    "    conf = float(confianca[i])\n",
    "    ax.text(x1, y1 - 5, f'{class_name}: {conf:.2f}', color='red', fontsize=10, backgroundcolor='white')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Faculdade",
   "language": "python",
   "name": "faculdade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
