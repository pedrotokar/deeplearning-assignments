{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa07f56",
   "metadata": {},
   "source": [
    "# **Programming Assignment 2 - Object Detection + Semantic Segmentation**\n",
    "\n",
    "#### **Professor**: Dário Oliveira  \n",
    "#### **Monitora**: Lívia Meinhardt\n",
    "\n",
    "\n",
    "O objetivo deste trabalho é construir um pipeline de visão computacional que primeiro **detecta** objetos em uma imagem e, em seguida, realiza a **segmentação semântica** em cada objeto detectado. Ou seja, uma segmentação de instância em duas etapas. \n",
    "\n",
    "Vocês irão construir e conectar dois modelos distintos:\n",
    "1.  Um **detector de objetos** (YOLO) para encontrar a localização dos objetos.\n",
    "2.  Um **segmentador semântico** (U-Net ou outro) para classificar os pixels dentro de cada objeto localizado.\n",
    "\n",
    "### **Instruções:**\n",
    "\n",
    "1. **Criação de um Dataset**:  \n",
    "Vocês usarão o dataset **[PASCAL VOC 2012](https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset)**. Este conjunto de dados é ideal porque fornece anotações tanto para **caixas delimitadoras** (*bounding boxes*), para a tarefa de detecção, quanto para **máscaras de segmentação** a nível de pixel. \n",
    "\n",
    "\n",
    "2. **Implementação e Treinamento da YOLO:**\n",
    "Sua primeira tarefa é fazer a implementação da YOLOv3, vista em aula. Treine um modelo que recebe uma imagem como entrada e retorna uma lista de predições, onde cada predição contém `(caixa_delimitadora, classe, score_de_confiança)`. Meça o desempenho do seu detector usando a métrica **Mean Average Precision (mAP)**.\n",
    "\n",
    "\n",
    "3. **Treinar o Segmentador Semântico:**\n",
    "Sua segunda tarefa é treinar um modelo **U-Net** ou outro de sua preferência para realizar a segmentação. O ponto crucial é que vocês não irão treinar a U-Net com imagens inteiras. Em vez disso, vocês a treinarão com **recortes de imagem (*patches*)** gerados a partir das caixas delimitadoras de referência (*ground-truth*) do dataset. Meça o desempenho do seu segmentador usando a métrica **Average Precision (AP).**\n",
    "\n",
    "4. **Construir o Pipeline de Inferência:**\n",
    "    Agora, conecte seus dois modelos treinados. Esta parte envolve escrever um script que executa a tarefa completa de ponta a ponta.\n",
    "\n",
    "    1.  **Detectar**: Use uma nova imagem de teste e passe-a pelo seu modelo **YOLOv3** treinado para obter uma lista de caixas delimitadoras preditas.\n",
    "    2.  **Recortar**: Para cada predição de alta confiança do YOLO, **recorte o *patch* da imagem** definido pela caixa delimitadora.\n",
    "    3.  **Segmentar**: Passe cada *patch* recortado pelo seu modelo **U-Net** treinado para obter uma máscara de segmentação para aquele objeto específico.\n",
    "    4.  **Combinar**: Crie uma imagem em branco (preta) do mesmo tamanho da imagem original. \"Costure\" cada máscara gerada de volta nesta imagem em branco, na sua localização original da caixa delimitadora.\n",
    "    5.  **Visualizar**: Sobreponha a máscara final combinada na imagem original para criar uma visualização final mostrando todos os objetos detectados e segmentados.\n",
    "\n",
    "\n",
    "5. **Compare com um método *end-to-end:***\n",
    "Por fim, faça *fine-tuning* do [**Mask R-CNN**](https://docs.pytorch.org/vision/main/models/mask_rcnn.html), um um modelo de segmentação de instância de ponta a ponta (*end-to-end*). Compare o desempenho com o seu pipeline de dois estágios e discuta as diferenças. \n",
    "\n",
    "\n",
    "### **Entrega:**\n",
    "\n",
    "Você deve enviar:\n",
    "\n",
    "1.  Um **Jupyter Notebook** contendo todo o seu código.\n",
    "2.  Os **pesos treinados** tanto do seu detector YOLOv3 quanto do seu segmentador U-Net.\n",
    "3.  Um **relatório ou apresentação** que discuta os desafios e resultados dos seus experimentos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd05a4",
   "metadata": {},
   "source": [
    "## Preparando o dataset pra detecção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "gopalbhattrai_pascal_voc_2012_dataset_path = kagglehub.dataset_download('gopalbhattrai/pascal-voc-2012-dataset')\n",
    "\n",
    "print('Data source import complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gopalbhattrai_pascal_voc_2012_dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d42e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision.io import decode_image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead, FCNHead\n",
    "\n",
    "#from torchtune.datasets import ConcatDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"using device '{device}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Define dataset root\n",
    "original_dataset_path = 'data'\n",
    "yolo_dataset_path = 'yolo_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e97baaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "The directory /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/JPEGImages or /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/Annotations does not exist. Please verify the dataset path.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m annotations_dir = os.path.join(original_dataset_path, \u001b[33m'\u001b[39m\u001b[33mVOC2012_train_val\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAnnotations\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(jpeg_images_dir) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(annotations_dir):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjpeg_images_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mannotations_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist. Please verify the dataset path.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m image_filenames = os.listdir(jpeg_images_dir)\n\u001b[32m     16\u001b[39m image_ids = [os.path.splitext(filename)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m image_filenames \u001b[38;5;28;01mif\u001b[39;00m filename.endswith(\u001b[33m'\u001b[39m\u001b[33m.jpg\u001b[39m\u001b[33m'\u001b[39m)]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: The directory /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/JPEGImages or /home/pedro/Modelos/Faculdade/DL/Assignment 2/data/VOC2012_train_val/Annotations does not exist. Please verify the dataset path."
     ]
    }
   ],
   "source": [
    "yolo_dirs = [\n",
    "    os.path.join(yolo_dataset_path, 'images', 'train'),\n",
    "    os.path.join(yolo_dataset_path, 'images', 'val'),\n",
    "    os.path.join(yolo_dataset_path, 'labels', 'train'),\n",
    "    os.path.join(yolo_dataset_path, 'labels', 'val')\n",
    "]\n",
    "\n",
    "for yolo_dir in yolo_dirs:\n",
    "    os.makedirs(yolo_dir, exist_ok=True)\n",
    "\n",
    "jpeg_images_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'JPEGImages')\n",
    "annotations_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'Annotations')\n",
    "if not os.path.exists(jpeg_images_dir) or not os.path.exists(annotations_dir):\n",
    "    raise FileNotFoundError(f\"The directory {jpeg_images_dir} or {annotations_dir} does not exist. Please verify the dataset path.\")\n",
    "image_filenames = os.listdir(jpeg_images_dir)\n",
    "image_ids = [os.path.splitext(filename)[0] for filename in image_filenames if filename.endswith('.jpg')]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids)\n",
    "split_index = int(0.8 * len(image_ids)) #Spliting the dataset 80% for training, 20% for validation\n",
    "train_ids = image_ids[:split_index] #taking the first 80% pictures\n",
    "val_ids = image_ids[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cb7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "#this fucntion converts PASCAL_VOC annotations to YOLO format\n",
    "def create_yolo_annotation(xml_file_path, yolo_label_path, label_dict):\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    annotations = [] #list that will store the converted YOLO annotations.\n",
    "\n",
    "    img_width = int(root.find('size/width').text)\n",
    "    img_height = int(root.find('size/height').text)\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        if label not in label_dict:\n",
    "            continue\n",
    "        label_idx = label_dict[label]\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = float(bndbox.find('xmin').text)\n",
    "        ymin = float(bndbox.find('ymin').text)\n",
    "        xmax = float(bndbox.find('xmax').text)\n",
    "        ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "        # this is YOLOv8 annotation format: label x_center y_center width height (normalized)\n",
    "        x_center = ((xmin + xmax) / 2) / img_width\n",
    "        y_center = ((ymin + ymax) / 2) / img_height\n",
    "        width = (xmax - xmin) / img_width\n",
    "        height = (ymax - ymin) / img_height\n",
    "\n",
    "        annotations.append(f\"{label_idx} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "\n",
    "    #annotations to the label file\n",
    "    with open(yolo_label_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "    'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9,\n",
    "    'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,\n",
    "    'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19\n",
    "}\n",
    "\n",
    "for image_set, ids in [('train', train_ids), ('val', val_ids)]:\n",
    "    for img_id in ids:\n",
    "        img_src_path = os.path.join(jpeg_images_dir, f'{img_id}.jpg')\n",
    "        label_dst_path = os.path.join(yolo_dataset_path, 'labels', image_set, f'{img_id}.txt')\n",
    "\n",
    "        # Create the YOLO annotation file\n",
    "        xml_file_path = os.path.join(annotations_dir, f'{img_id}.xml')\n",
    "        if not os.path.exists(xml_file_path):\n",
    "            print(f\"Warning: Annotation {xml_file_path} not found, skipping.\")\n",
    "            continue\n",
    "        create_yolo_annotation(xml_file_path, label_dst_path, label_dict)\n",
    "\n",
    "        # Copy the image to the new YOLO dataset structure\n",
    "        img_dst_path = os.path.join(yolo_dataset_path, 'images', image_set, f'{img_id}.jpg')\n",
    "        shutil.copy(img_src_path, img_dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ae84d",
   "metadata": {},
   "source": [
    "### Testando na YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30200929",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "nc: {len(label_dict)}\n",
    "names: {list(label_dict.keys())}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(yolo_dataset_path, 'data.yaml'), 'w') as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79713165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # Use the YOLOv8 nano model\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=os.path.join(yolo_dataset_path, 'data.yaml'),  # Path to dataset config file\n",
    "    epochs=2,  # Number of epochs\n",
    "    imgsz=640,  # Image size\n",
    "    batch=16,  # Batch size\n",
    "    name='yolov8_test'  # Experiment name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source=os.path.join(yolo_dataset_path, 'images/val/'), save=True)  # Run inference on validation set\n",
    "\n",
    "# Display predictions\n",
    "for result in results:\n",
    "    result.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5882cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_conf = 0\n",
    "best_result = None\n",
    "\n",
    "for result in results:    \n",
    "        if (result.boxes.conf.mean().item() > max_conf) and (result.boxes.conf.mean().item() < 0.98) and (result.boxes.conf.shape[0] > 2):\n",
    "            max_conf = result.boxes.conf.mean().item()\n",
    "            best_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "img = best_result.orig_img\n",
    "boxes = best_result.boxes.xyxy.cpu().numpy()  # xyxy format\n",
    "classe = best_result.boxes.cls\n",
    "confianca = best_result.boxes.conf\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = box[:4]\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    # Add class and confidence\n",
    "    class_idx = int(classe[i])\n",
    "    class_name = best_result.names[class_idx] if hasattr(best_result, 'names') else str(class_idx)\n",
    "    conf = float(confianca[i])\n",
    "    ax.text(x1, y1 - 5, f'{class_name}: {conf:.2f}', color='red', fontsize=10, backgroundcolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070581c2",
   "metadata": {},
   "source": [
    "# Implementando YOLO V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98f21f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": []\n",
    "        }\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        for X, y in progress_bar:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "            pred = self.model(X)\n",
    "            loss = self.criterion(pred, y)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "            \n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(self.train_loader.dataset)\n",
    "        return epoch_loss\n",
    "\n",
    "    def validate_one_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(self.val_loader, desc=\"Validating\")\n",
    "            for X, y in progress_bar:\n",
    "                X, y = X.to(self.device), y.to(self.device)\n",
    "\n",
    "                pred = self.model(X)\n",
    "                loss = self.criterion(pred, y)\n",
    "\n",
    "                running_loss += loss.item() * X.size(0)\n",
    "\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(self.val_loader.dataset)\n",
    "        return epoch_loss\n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = self.train_one_epoch()\n",
    "            val_loss = self.validate_one_epoch()\n",
    "\n",
    "            self.history[\"train_loss\"].append(train_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a911c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Literal, Union\n",
    "\n",
    "@dataclass\n",
    "class ConvBlockSpec:\n",
    "    out_channels: int\n",
    "    kernel_size: int\n",
    "    stride: int\n",
    "    padding: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.padding = 1 if self.kernel_size == 3 else 0\n",
    "\n",
    "@dataclass\n",
    "class ResidualBlockSpec:\n",
    "    num_repeats: int\n",
    "    channels: int\n",
    "\n",
    "@dataclass\n",
    "class ScaleBlockSpec:\n",
    "    from_layer: int = -1\n",
    "\n",
    "@dataclass\n",
    "class UpsampleBlockSpec:\n",
    "    from_layer: int = -1\n",
    "\n",
    "config = [\n",
    "    #===DARKNET-53===\n",
    "    ConvBlockSpec(32, 3, 1),\n",
    "    ConvBlockSpec(64, 3, 2),\n",
    "    ResidualBlockSpec(1, 64),\n",
    "    \n",
    "    ConvBlockSpec(128, 3, 2),\n",
    "    ResidualBlockSpec(2, 128),\n",
    "    \n",
    "    ConvBlockSpec(256, 3, 2),\n",
    "    ResidualBlockSpec(8, 256),\n",
    "    \n",
    "    ConvBlockSpec(512, 3, 2),\n",
    "    ResidualBlockSpec(8, 512),\n",
    "\n",
    "    ConvBlockSpec(1024, 3, 2),\n",
    "    ResidualBlockSpec(4, 1024),\n",
    "    #===DARKNET-53===\n",
    "\n",
    "    ConvBlockSpec(512, 1, 1),\n",
    "    ConvBlockSpec(1024, 1, 1),\n",
    "    ScaleBlockSpec(),\n",
    "    \n",
    "    ConvBlockSpec(256, 1, 1),\n",
    "    UpsampleBlockSpec(),\n",
    "    \n",
    "    ConvBlockSpec(256, 1, 1),\n",
    "    ConvBlockSpec(512, 1, 1),\n",
    "    ScaleBlockSpec(),\n",
    "\n",
    "    ConvBlockSpec(128, 1, 1),\n",
    "    UpsampleBlockSpec(),\n",
    "\n",
    "    ConvBlockSpec(128, 1, 1),\n",
    "    ConvBlockSpec(256, 1, 1),\n",
    "    ScaleBlockSpec(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c656a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 64, 208, 208])\n",
      "torch.Size([10, 128, 104, 104])\n",
      "torch.Size([10, 128, 104, 104])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 256, 52, 52])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 512, 26, 26])\n",
      "torch.Size([10, 1024, 13, 13])\n",
      "torch.Size([10, 1024, 13, 13])\n",
      "torch.Size([10, 1024, 13, 13])\n",
      "torch.Size([10, 1024, 13, 13])\n",
      "scale\n",
      "concat\n",
      "scale\n",
      "concat\n",
      "scale\n"
     ]
    }
   ],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        use_bias = not bn_act\n",
    "        layers = []\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, out_channels, bias=use_bias, **kwargs)\n",
    "        )\n",
    "\n",
    "        if bn_act:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residuals=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_residuals = use_residuals\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_repeats):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels//2, kernel_size=1),\n",
    "                    CNNBlock(channels//2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.block:\n",
    "            if self.use_residuals:\n",
    "                print(x.shape)\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, num_anchors=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2*in_channels,\n",
    "                self.num_anchors *(num_classes+5),\n",
    "                bn_act=False,\n",
    "                kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # [p, cx, cy, w, h, Ic1, ..., Icn] * num_anchors\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pred = self.pred(x)\n",
    "\n",
    "        # N, C, W, H\n",
    "        N, _, S, _ = pred.shape\n",
    "\n",
    "        pred = pred.reshape(N, self.num_anchors, self.num_classes+5, S, S)\n",
    "        pred = pred.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, config, num_anchors=3, in_channels=3, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.in_channels = 3\n",
    "        self.num_classes = 20\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.layers = self.gen_layers()\n",
    "\n",
    "    def gen_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for layer in self.config:\n",
    "            if isinstance(layer, ConvBlockSpec):\n",
    "                layers.append(CNNBlock(in_channels, **asdict(layer)))\n",
    "                in_channels = layer.out_channels\n",
    "            elif isinstance(layer, ResidualBlockSpec):\n",
    "                layers.append(ResidualBlock(**asdict(layer)))\n",
    "            elif isinstance(layer, ScaleBlockSpec):\n",
    "                layers += [\n",
    "                    ResidualBlock(in_channels, use_residuals=False, num_repeats=1),\n",
    "                    CNNBlock(in_channels, in_channels//2, kernel_size=1),\n",
    "                    ScaleBlock(in_channels//2, self.num_classes, num_anchors=self.num_anchors)\n",
    "                ]\n",
    "                in_channels = in_channels//2\n",
    "                \n",
    "            elif isinstance(layer, UpsampleBlockSpec):\n",
    "                layers.append(nn.Upsample(scale_factor=2))\n",
    "                in_channels = in_channels*3\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        preds = []\n",
    "        route_connections = []\n",
    "\n",
    "        #print(self.layers)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # print(isinstance(layer, CNNBlock))\n",
    "            # print(isinstance(layer, ResidualBlock))\n",
    "            # print(isinstance(layer, ScaleBlock))\n",
    "            # print()\n",
    "\n",
    "            if isinstance(layer, ScaleBlock):\n",
    "                print(\"scale\")\n",
    "                preds.append(layer(x))\n",
    "                continue\n",
    "            \n",
    "            x = layer(x)\n",
    "            \n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            if isinstance(layer, nn.Upsample):\n",
    "                print(\"concat\")\n",
    "                x = torch.cat([x, route_connections.pop()], dim=1)\n",
    "\n",
    "        return preds\n",
    "\n",
    "yolo = YOLOv3(config, num_anchors=2)\n",
    "x = torch.randn((10, 3, 416, 416))\n",
    "out = yolo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28e35f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "torch.Size([2, 26, 26, 25])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "for img in out[1]:\n",
    "    print(img.shape)\n",
    "    counter += 1\n",
    "\n",
    "print(counter)\n",
    "    #print(out[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25779a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecções encontradas: 1\n",
      "Detalhes da primeira detecção: {'box': [-271.99346923828125, -641.3070068359375, 572.1517944335938, 941.46533203125], 'class_id': 6, 'score': 0.5301257967948914}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision\n",
    "\n",
    "# --- PARÂMETROS E DADOS SIMULADOS ---\n",
    "# (Substitua estes pelos seus dados reais)\n",
    "\n",
    "# 1. Parâmetros do modelo e pós-processamento\n",
    "CONF_THRESHOLD = 0.5  # Confiança mínima para considerar uma detecção\n",
    "NMS_THRESHOLD = 0.4   # Limiar de IoU para a Supressão Não Máxima (NMS)\n",
    "IMG_SIZE = 416        # Tamanho da imagem que o modelo espera (ex: 416x416)\n",
    "NUM_CLASSES = 20      # Número de classes (ex: 20 para PASCAL VOC)\n",
    "\n",
    "# 2. Nomes das classes (exemplo com 20 classes do PASCAL VOC)\n",
    "CLASS_NAMES = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
    "    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]\n",
    "\n",
    "# 3. Âncoras usadas pela YOLOv3 para a escala de 52x52 (objetos pequenos)\n",
    "# (Estes valores são relativos ao tamanho da imagem, ex: 416)\n",
    "ANCHORS_52x52 = [(10, 13), (16, 30), (33, 23)]\n",
    "\n",
    "# 4. Criar uma imagem preta de exemplo\n",
    "# Na prática, você carregaria sua imagem com: cv2.imread(\"sua_imagem.jpg\")\n",
    "dummy_image = np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
    "\n",
    "# 5. Criar uma saída de modelo YOLO simulada (torch.Size([1, 3, 13, 13, 25]))\n",
    "# O formato é [batch_size, num_anchors, grid_h, grid_w, 5 + num_classes]\n",
    "# Usaremos uma grade 13x13 para simplificar a visualização\n",
    "GRID_SIZE = 13\n",
    "dummy_output = torch.zeros(1, 3, GRID_SIZE, GRID_SIZE, 5 + NUM_CLASSES)\n",
    "\n",
    "# ---- Vamos \"forçar\" uma detecção para ter algo para plotar ----\n",
    "# Classe 6: \"car\" | Âncora 1 | Célula da grade (4, 4)\n",
    "dummy_output[0, 1, 4, 4, 0] = 0.8   # tx (centro x) -> próximo ao centro da célula\n",
    "dummy_output[0, 1, 4, 4, 1] = 0.8   # ty (centro y) -> próximo ao centro da célula\n",
    "dummy_output[0, 1, 4, 4, 2] = 0.5   # tw (largura) -> um pouco maior que a âncora\n",
    "dummy_output[0, 1, 4, 4, 3] = 0.5   # th (altura) -> um pouco maior que a âncora\n",
    "dummy_output[0, 1, 4, 4, 4] = 0.98  # Confiança de que há um objeto (objectness)\n",
    "dummy_output[0, 1, 4, 4, 5 + 6] = 0.99 # Probabilidade da classe \"car\"\n",
    "\n",
    "\n",
    "# --- FUNÇÕES DE PROCESSAMENTO E PLOTAGEM ---\n",
    "\n",
    "def process_yolo_output(yolo_output, conf_threshold, nms_threshold, anchors):\n",
    "    \"\"\"\n",
    "    Processa a saída bruta da YOLO para obter as caixas finais.\n",
    "    \"\"\"\n",
    "    predictions = yolo_output.clone()\n",
    "    \n",
    "    # Obter dimensões\n",
    "    batch_size, num_anchors, grid_h, grid_w, num_attrs = predictions.shape\n",
    "    \n",
    "    # 1. Transformar as coordenadas da saída\n",
    "    # A saída da YOLO precisa ser convertida para coordenadas de imagem reais.\n",
    "    \n",
    "    # Criar uma grade de coordenadas (ex: [0,1,2...12])\n",
    "    grid_x = torch.arange(grid_w).repeat(grid_h, 1).view([1, 1, grid_h, grid_w]).float()\n",
    "    grid_y = torch.arange(grid_h).repeat(grid_w, 1).t().view([1, 1, grid_h, grid_w]).float()\n",
    "\n",
    "    # Converter âncoras para tensores\n",
    "    tensor_anchors = torch.tensor(anchors).float().view(1, num_anchors, 1, 1, 2)\n",
    "    \n",
    "    # Aplicar as fórmulas de transformação da YOLO\n",
    "    # Ativar coordenadas de centro (tx, ty) e confiança (obj) com sigmoide\n",
    "    predictions[..., 0:2] = torch.sigmoid(predictions[..., 0:2])  # Centro x, y\n",
    "    predictions[..., 4] = torch.sigmoid(predictions[..., 4])      # Confiança (objectness)\n",
    "    \n",
    "    # Adicionar o offset da grade para obter a posição na imagem\n",
    "    predictions[..., 0] += grid_x\n",
    "    predictions[..., 1] += grid_y\n",
    "    \n",
    "    # Aplicar a fórmula da exponencial para largura e altura\n",
    "    predictions[..., 2:4] = torch.exp(predictions[..., 2:4]) * tensor_anchors\n",
    "    \n",
    "    # Ativar as probabilidades de classe com sigmoide\n",
    "    predictions[..., 5:] = torch.sigmoid(predictions[..., 5:])\n",
    "    \n",
    "    # Escalonar as coordenadas para o tamanho real da imagem (ex: 416x416)\n",
    "    stride = IMG_SIZE / grid_w\n",
    "    predictions[..., :4] *= stride\n",
    "    \n",
    "    # 2. Filtrar as previsões\n",
    "    # Transformar de [batch, anchors, grid, grid, attrs] para uma lista de caixas\n",
    "    predictions = predictions.view(batch_size, -1, num_attrs) # [-1] achata as dimensões de âncora e grade\n",
    "    \n",
    "    output_boxes = []\n",
    "    for batch_i in range(batch_size):\n",
    "        preds = predictions[batch_i]\n",
    "        \n",
    "        # Filtrar por confiança de objeto\n",
    "        mask = preds[:, 4] > conf_threshold\n",
    "        preds = preds[mask]\n",
    "        \n",
    "        if not preds.size(0):\n",
    "            continue\n",
    "            \n",
    "        # Obter a classe com maior probabilidade\n",
    "        class_scores, class_ids = torch.max(preds[:, 5:], 1)\n",
    "        \n",
    "        # Juntar tudo em um formato [x1, y1, x2, y2, obj_conf, class_score, class_id]\n",
    "        # Converter (centro_x, centro_y, w, h) para (x1, y1, x2, y2)\n",
    "        box_center_x, box_center_y, box_w, box_h = preds[:, 0], preds[:, 1], preds[:, 2], preds[:, 3]\n",
    "        x1 = box_center_x - box_w / 2\n",
    "        y1 = box_center_y - box_h / 2\n",
    "        x2 = box_center_x + box_w / 2\n",
    "        y2 = box_center_y + box_h / 2\n",
    "        \n",
    "        # 3. Aplicar Supressão Não Máxima (NMS)\n",
    "        # NMS remove caixas redundantes para o mesmo objeto\n",
    "        boxes_for_nms = torch.stack([x1, y1, x2, y2], dim=1)\n",
    "        scores = preds[:, 4] * class_scores # Usar a confiança do objeto e da classe\n",
    "        \n",
    "        indices = torchvision.ops.nms(boxes_for_nms, scores, nms_threshold)\n",
    "        \n",
    "        # Selecionar apenas as caixas que sobreviveram ao NMS\n",
    "        final_boxes = []\n",
    "        for i in indices:\n",
    "            final_boxes.append({\n",
    "                \"box\": [x1[i].item(), y1[i].item(), x2[i].item(), y2[i].item()],\n",
    "                \"class_id\": class_ids[i].item(),\n",
    "                \"score\": scores[i].item()\n",
    "            })\n",
    "        output_boxes.append(final_boxes)\n",
    "\n",
    "    return output_boxes[0] # Retornando apenas o resultado do primeiro item do batch\n",
    "\n",
    "def draw_boxes(image, boxes, class_names):\n",
    "    \"\"\"\n",
    "    Desenha as caixas delimitadoras na imagem.\n",
    "    \"\"\"\n",
    "    vis_image = image.copy()\n",
    "    \n",
    "    for box_info in boxes:\n",
    "        box = box_info['box']\n",
    "        class_id = box_info['class_id']\n",
    "        score = box_info['score']\n",
    "        \n",
    "        # Coordenadas como inteiros\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        \n",
    "        # Obter nome e cor da classe\n",
    "        class_name = class_names[class_id]\n",
    "        color = (255, 255, 255) # Verde para a caixa\n",
    "        \n",
    "        # Desenhar a caixa\n",
    "        cv2.rectangle(vis_image, (x1, y1), (x2, y2), color, 2)\n",
    "        \n",
    "        # Preparar o texto (Classe + Confiança)\n",
    "        label = f\"{class_name}: {score:.2f}\"\n",
    "        \n",
    "        # Desenhar um fundo para o texto\n",
    "        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
    "        cv2.rectangle(vis_image, (x1, y1 - h - 5), (x1 + w, y1), color, -1)\n",
    "        \n",
    "        # Escrever o texto\n",
    "        cv2.putText(vis_image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
    "\n",
    "    return vis_image\n",
    "\n",
    "# --- EXECUÇÃO PRINCIPAL ---\n",
    "\n",
    "# 1. Processar a saída simulada da YOLO\n",
    "final_detections = process_yolo_output(\n",
    "    yolo_output=dummy_output,\n",
    "    conf_threshold=CONF_THRESHOLD,\n",
    "    nms_threshold=NMS_THRESHOLD,\n",
    "    anchors=ANCHORS_52x52  # Use as âncoras corretas para a escala\n",
    ")\n",
    "\n",
    "# 2. Desenhar as caixas na imagem\n",
    "image_with_boxes = draw_boxes(dummy_image, final_detections, CLASS_NAMES)\n",
    "\n",
    "# 3. Exibir a imagem\n",
    "print(f\"Detecções encontradas: {len(final_detections)}\")\n",
    "if len(final_detections) > 0:\n",
    "    print(\"Detalhes da primeira detecção:\", final_detections[0])\n",
    "\n",
    "cv2.imshow(\"YOLO Output Visualization\", image_with_boxes)\n",
    "cv2.waitKey(0)  # Espera uma tecla ser pressionada para fechar\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca9d1507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[1][0,0,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ff581",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ResidualBlock(channels=32)\n",
    "x = torch.randn((2, 32, 416, 416))\n",
    "res(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2123c8b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722d4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studies",
   "language": "python",
   "name": "studies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
