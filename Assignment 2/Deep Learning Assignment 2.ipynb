{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfa07f56",
   "metadata": {
    "id": "cfa07f56"
   },
   "source": [
    "# **Programming Assignment 2 - Object Detection + Semantic Segmentation**\n",
    "\n",
    "#### **Professor**: Dário Oliveira  \n",
    "#### **Monitora**: Lívia Meinhardt\n",
    "\n",
    "\n",
    "O objetivo deste trabalho é construir um pipeline de visão computacional que primeiro **detecta** objetos em uma imagem e, em seguida, realiza a **segmentação semântica** em cada objeto detectado. Ou seja, uma segmentação de instância em duas etapas.\n",
    "\n",
    "Vocês irão construir e conectar dois modelos distintos:\n",
    "1.  Um **detector de objetos** (YOLO) para encontrar a localização dos objetos.\n",
    "2.  Um **segmentador semântico** (U-Net ou outro) para classificar os pixels dentro de cada objeto localizado.\n",
    "\n",
    "### **Instruções:**\n",
    "\n",
    "1. **Criação de um Dataset**:  \n",
    "Vocês usarão o dataset **[PASCAL VOC 2012](https://www.kaggle.com/datasets/gopalbhattrai/pascal-voc-2012-dataset)**. Este conjunto de dados é ideal porque fornece anotações tanto para **caixas delimitadoras** (*bounding boxes*), para a tarefa de detecção, quanto para **máscaras de segmentação** a nível de pixel.\n",
    "\n",
    "\n",
    "2. **Implementação e Treinamento da YOLO:**\n",
    "Sua primeira tarefa é fazer a implementação da YOLOv3, vista em aula. Treine um modelo que recebe uma imagem como entrada e retorna uma lista de predições, onde cada predição contém `(caixa_delimitadora, classe, score_de_confiança)`. Meça o desempenho do seu detector usando a métrica **Mean Average Precision (mAP)**.\n",
    "\n",
    "\n",
    "3. **Treinar o Segmentador Semântico:**\n",
    "Sua segunda tarefa é treinar um modelo **U-Net** ou outro de sua preferência para realizar a segmentação. O ponto crucial é que vocês não irão treinar a U-Net com imagens inteiras. Em vez disso, vocês a treinarão com **recortes de imagem (*patches*)** gerados a partir das caixas delimitadoras de referência (*ground-truth*) do dataset. Meça o desempenho do seu segmentador usando a métrica **Average Precision (AP).**\n",
    "\n",
    "4. **Construir o Pipeline de Inferência:**\n",
    "    Agora, conecte seus dois modelos treinados. Esta parte envolve escrever um script que executa a tarefa completa de ponta a ponta.\n",
    "\n",
    "    1.  **Detectar**: Use uma nova imagem de teste e passe-a pelo seu modelo **YOLOv3** treinado para obter uma lista de caixas delimitadoras preditas.\n",
    "    2.  **Recortar**: Para cada predição de alta confiança do YOLO, **recorte o *patch* da imagem** definido pela caixa delimitadora.\n",
    "    3.  **Segmentar**: Passe cada *patch* recortado pelo seu modelo **U-Net** treinado para obter uma máscara de segmentação para aquele objeto específico.\n",
    "    4.  **Combinar**: Crie uma imagem em branco (preta) do mesmo tamanho da imagem original. \"Costure\" cada máscara gerada de volta nesta imagem em branco, na sua localização original da caixa delimitadora.\n",
    "    5.  **Visualizar**: Sobreponha a máscara final combinada na imagem original para criar uma visualização final mostrando todos os objetos detectados e segmentados.\n",
    "\n",
    "\n",
    "5. **Compare com um método *end-to-end:***\n",
    "Por fim, faça *fine-tuning* do [**Mask R-CNN**](https://docs.pytorch.org/vision/main/models/mask_rcnn.html), um um modelo de segmentação de instância de ponta a ponta (*end-to-end*). Compare o desempenho com o seu pipeline de dois estágios e discuta as diferenças.\n",
    "\n",
    "\n",
    "### **Entrega:**\n",
    "\n",
    "Você deve enviar:\n",
    "\n",
    "1.  Um **Jupyter Notebook** contendo todo o seu código.\n",
    "2.  Os **pesos treinados** tanto do seu detector YOLOv3 quanto do seu segmentador U-Net.\n",
    "3.  Um **relatório ou apresentação** que discuta os desafios e resultados dos seus experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cd05a4",
   "metadata": {
    "id": "13cd05a4"
   },
   "source": [
    "## Preparando o dataset pra detecção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf2ddd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T15:12:45.098440Z",
     "iopub.status.busy": "2025-09-20T15:12:45.097748Z",
     "iopub.status.idle": "2025-09-20T15:12:45.481515Z",
     "shell.execute_reply": "2025-09-20T15:12:45.480754Z",
     "shell.execute_reply.started": "2025-09-20T15:12:45.098409Z"
    },
    "id": "8bf2ddd4",
    "outputId": "724116ea-0f2e-423b-e70b-49ae4520913a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "gopalbhattrai_pascal_voc_2012_dataset_path = kagglehub.dataset_download('gopalbhattrai/pascal-voc-2012-dataset')\n",
    "\n",
    "print('Data source import complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832b87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:17:26.546032Z",
     "iopub.status.busy": "2025-09-20T15:17:26.545751Z",
     "iopub.status.idle": "2025-09-20T15:17:26.549771Z",
     "shell.execute_reply": "2025-09-20T15:17:26.549109Z",
     "shell.execute_reply.started": "2025-09-20T15:17:26.546013Z"
    },
    "id": "3832b87f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "# Define dataset root\n",
    "original_dataset_path = gopalbhattrai_pascal_voc_2012_dataset_path\n",
    "yolo_dataset_path = 'yolo_dataset'\n",
    "unet_dataset_path = 'unet_dataset'\n",
    "mask_dataset_path = 'mask_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97baaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:17:28.531234Z",
     "iopub.status.busy": "2025-09-20T15:17:28.530625Z",
     "iopub.status.idle": "2025-09-20T15:17:28.726959Z",
     "shell.execute_reply": "2025-09-20T15:17:28.726168Z",
     "shell.execute_reply.started": "2025-09-20T15:17:28.531195Z"
    },
    "id": "6e97baaf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo_dirs = [\n",
    "    os.path.join(yolo_dataset_path, 'images', 'train'),\n",
    "    os.path.join(yolo_dataset_path, 'images', 'val'),\n",
    "    os.path.join(yolo_dataset_path, 'labels', 'train'),\n",
    "    os.path.join(yolo_dataset_path, 'labels', 'val')\n",
    "]\n",
    "\n",
    "for yolo_dir in yolo_dirs:\n",
    "    os.makedirs(yolo_dir, exist_ok=True)\n",
    "\n",
    "jpeg_images_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'VOC2012_train_val', 'JPEGImages')\n",
    "annotations_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'VOC2012_train_val', 'Annotations')\n",
    "\n",
    "if not os.path.exists(jpeg_images_dir) or not os.path.exists(annotations_dir):\n",
    "    raise FileNotFoundError(\n",
    "        f\"\"\"\n",
    "        The directory {jpeg_images_dir} or {annotations_dir} does not exist. Please verify the dataset path.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "image_filenames = os.listdir(jpeg_images_dir)\n",
    "image_ids = [os.path.splitext(filename)[0] for filename in image_filenames if filename.endswith('.jpg')]\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(image_ids)\n",
    "split_index = int(0.8 * len(image_ids)) # Spliting the dataset 80% for training, 20% for validation\n",
    "train_ids = image_ids[:split_index]     # Taking the first 80% pictures\n",
    "val_ids = image_ids[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_dirs = [\n",
    "    os.path.join(unet_dataset_path, 'images', 'train'),\n",
    "    os.path.join(unet_dataset_path, 'images', 'val'),\n",
    "    os.path.join(unet_dataset_path, 'labels', 'train'),\n",
    "    os.path.join(unet_dataset_path, 'labels', 'val')\n",
    "]\n",
    "\n",
    "segmentation_dir = os.path.join(original_dataset_path, 'VOC2012_train_val', 'VOC2012_train_val', 'SegmentationObject')\n",
    "\n",
    "for unet_dir in unet_dirs:\n",
    "    os.makedirs(unet_dir, exist_ok=True)\n",
    "\n",
    "segmentation_filenames = os.listdir(segmentation_dir)\n",
    "segmentation_ids = [os.path.splitext(filename)[0] for filename in segmentation_filenames if filename.endswith('.png')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0cb7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:17:30.408660Z",
     "iopub.status.busy": "2025-09-20T15:17:30.408397Z",
     "iopub.status.idle": "2025-09-20T15:17:30.415219Z",
     "shell.execute_reply": "2025-09-20T15:17:30.414458Z",
     "shell.execute_reply.started": "2025-09-20T15:17:30.408641Z"
    },
    "id": "be0cb7f8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# This function converts PASCAL_VOC annotations to YOLO format\n",
    "def create_yolo_annotation(xml_file_path, yolo_label_path, label_dict):\n",
    "    classes_list = []\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    annotations = [] # List that will store the converted YOLO annotations\n",
    "    crop_boxes = []\n",
    "\n",
    "    img_width = int(root.find('size/width').text)\n",
    "    img_height = int(root.find('size/height').text)\n",
    "\n",
    "\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        if label not in label_dict:\n",
    "            continue\n",
    "        label_idx = label_dict[label]\n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = float(bndbox.find('xmin').text)\n",
    "        ymin = float(bndbox.find('ymin').text)\n",
    "        xmax = float(bndbox.find('xmax').text)\n",
    "        ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "        # This is YOLOv8 annotation format: label x_center y_center width height (normalized)\n",
    "        x_center = ((xmin + xmax) / 2) / img_width\n",
    "        y_center = ((ymin + ymax) / 2) / img_height\n",
    "        width = (xmax - xmin) / img_width\n",
    "        height = (ymax - ymin) / img_height\n",
    "\n",
    "        annotations.append(f\"{label_idx} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "        crop_boxes.append((xmin, ymin, xmax, ymax))\n",
    "        classes_list.append(label_idx)\n",
    "\n",
    "    # Annotations to the label file\n",
    "    with open(yolo_label_path, 'w') as f:\n",
    "        f.write(\"\\n\".join(annotations))\n",
    "\n",
    "    return crop_boxes, classes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed3698",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T15:17:32.226111Z",
     "iopub.status.busy": "2025-09-20T15:17:32.225833Z",
     "iopub.status.idle": "2025-09-20T15:21:44.870415Z",
     "shell.execute_reply": "2025-09-20T15:21:44.869670Z",
     "shell.execute_reply.started": "2025-09-20T15:17:32.226092Z"
    },
    "id": "b9ed3698",
    "outputId": "7bb720e3-6241-4fd8-cf24-a0f4332afa6b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4,\n",
    "    'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9,\n",
    "    'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14,\n",
    "    'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19\n",
    "}\n",
    "\n",
    "crop_boxes = dict()\n",
    "classes_boxes = dict()\n",
    "\n",
    "for image_set, ids in [('train', train_ids), ('val', val_ids)]:\n",
    "    ids.sort()\n",
    "    for img_id in tqdm(ids, desc=f\"Processing {image_set}\"):\n",
    "        img_src_path = os.path.join(jpeg_images_dir, f'{img_id}.jpg')\n",
    "        label_dst_path = os.path.join(yolo_dataset_path, 'labels', image_set, f'{img_id}.txt')\n",
    "        \n",
    "        # Create the YOLO annotation file\n",
    "        xml_file_path = os.path.join(annotations_dir, f'{img_id}.xml')\n",
    "        if not os.path.exists(xml_file_path):\n",
    "            print(f\"Warning: Annotation {xml_file_path} not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        crop_boxes[img_id], classes_boxes[img_id] = create_yolo_annotation(xml_file_path, label_dst_path, label_dict)\n",
    "\n",
    "        # Copy the image to the new YOLO dataset structure\n",
    "        img_dst_path = os.path.join(yolo_dataset_path, 'images', image_set, f'{img_id}.jpg')\n",
    "\n",
    "        shutil.copy(img_src_path, img_dst_path)\n",
    "\n",
    "gt_seg_counter = 0\n",
    "for key in segmentation_ids:\n",
    "    gt_seg_counter += len(crop_boxes[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(segmentation_ids)\n",
    "split_index_seg = int(0.8 * gt_seg_counter) # Spliting the dataset 80% for training, 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c80caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_colors = {}\n",
    "split_counter = 0\n",
    "for img_id in segmentation_ids:\n",
    "    img_src_path = os.path.join(jpeg_images_dir, f\"{img_id}.jpg\")\n",
    "    seg_src_path = os.path.join(segmentation_dir, f\"{img_id}.png\")\n",
    "    original_img = Image.open(img_src_path)\n",
    "    original_seg = Image.open(seg_src_path)\n",
    "\n",
    "    for i, crop_box in enumerate(crop_boxes[img_id]):\n",
    "        cropped_img = original_img.crop(crop_box)\n",
    "        cropped_seg = original_seg.crop(crop_box)\n",
    "\n",
    "        cropped_img_filename = f\"{img_id}_{i}_{classes_boxes[img_id][i]}.jpg\"\n",
    "        cropped_seg_filename = f\"{img_id}_{i}_{classes_boxes[img_id][i]}.png\"\n",
    "\n",
    "        place = \"train\" if split_counter < split_index_seg else \"val\"\n",
    "        split_counter += 1\n",
    "\n",
    "        output_img_path = os.path.join(unet_dataset_path, \"images\", place, cropped_img_filename)\n",
    "        output_seg_path = os.path.join(unet_dataset_path, \"labels\", place, cropped_seg_filename)\n",
    "\n",
    "        cropped_img.save(output_img_path)\n",
    "        cropped_seg.save(output_seg_path)\n",
    "\n",
    "        img_rgb = cropped_seg.convert(\"RGB\")\n",
    "        colors = img_rgb.getcolors(img_rgb.size[0] * img_rgb.size[1])\n",
    "            \n",
    "        if colors:\n",
    "            most_frequent_color = sorted(colors, key=lambda item: item[0], reverse=True)\n",
    "            if most_frequent_color[0][1] != (0,0,0):\n",
    "                most_frequent_colors[img_id] = most_frequent_color[0][1]\n",
    "            else:\n",
    "                most_frequent_colors[img_id] = most_frequent_color[1][1]\n",
    "        else:\n",
    "            most_frequent_colors[img_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f891f7",
   "metadata": {},
   "source": [
    "## Dados para a MaskR-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c917fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dirs = [\n",
    "    os.path.join(mask_dataset_path, 'images', 'train'),\n",
    "    os.path.join(mask_dataset_path, 'images', 'val'),\n",
    "    os.path.join(mask_dataset_path, 'labels', 'train'),\n",
    "    os.path.join(mask_dataset_path, 'labels', 'val'),\n",
    "    os.path.join(mask_dataset_path, 'masks', 'train'),\n",
    "    os.path.join(mask_dataset_path, 'masks', 'val'),\n",
    "]\n",
    "\n",
    "for mask_dir in mask_dirs:\n",
    "    os.makedirs(mask_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c19837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_counter = 0\n",
    "place = \"train\"\n",
    "\n",
    "for img_id in tqdm(segmentation_ids):\n",
    "    img_src_path = os.path.join(jpeg_images_dir, f\"{img_id}.jpg\")\n",
    "    seg_src_path = os.path.join(segmentation_dir, f\"{img_id}.png\")\n",
    "\n",
    "    original_img = Image.open(img_src_path)\n",
    "    original_seg = Image.open(seg_src_path)\n",
    "    original_rgb = original_seg.convert(\"RGB\")\n",
    "    \n",
    "    output_img_path = os.path.join(mask_dataset_path, \"images\", place, f\"{img_id}.jpg\")\n",
    "    original_img.save(output_img_path)\n",
    "\n",
    "    started_iter = 0\n",
    "    for i, crop_box in enumerate(crop_boxes[img_id]):\n",
    "        place = \"train\" if ((split_counter < split_index_seg) or (started_iter)) else \"val\"\n",
    "        split_counter += 1\n",
    "        started_iter = 1\n",
    "\n",
    "        cropped_seg = original_seg.crop(crop_box)\n",
    "        \n",
    "        img_rgb = cropped_seg.convert(\"RGB\")\n",
    "        colors = img_rgb.getcolors(img_rgb.size[0] * img_rgb.size[1])\n",
    "        \n",
    "        most_frequent_colors = sorted(colors, key = lambda item: item[0], reverse = True)\n",
    "            \n",
    "        try:\n",
    "            if most_frequent_colors[0][1] != (0, 0, 0) and most_frequent_colors[0][1] != (224, 224, 192):\n",
    "                r, g, b = most_frequent_colors[0][1]\n",
    "            elif most_frequent_colors[1][1] != (0, 0, 0) and most_frequent_colors[1][1] != (224, 224, 192):\n",
    "                r, g, b = most_frequent_colors[1][1]\n",
    "            else:\n",
    "                r, g, b = most_frequent_colors[2][1]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        img_array = np.array(original_rgb)\n",
    "        mask = np.all(img_array == (r, g, b), axis=2)\n",
    "        out_array = np.zeros_like(img_array)\n",
    "        out_array[mask] = [255, 255, 255]\n",
    "\n",
    "        binary_mask_img = Image.fromarray(out_array)\n",
    "        output_mask_path = os.path.join(mask_dataset_path, \"masks\", place, f\"{img_id}_{i}.png\")\n",
    "        binary_mask_img.save(output_mask_path)\n",
    "\n",
    "        output_box_path = os.path.join(mask_dataset_path, \"labels\", place, f\"{img_id}_{i}.txt\")\n",
    "        with open(output_box_path, \"w\") as f:\n",
    "            f.write(f\"{classes_boxes[img_id][i]} {crop_box[0]} {crop_box[1]} {crop_box[2]} {crop_box[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34ae84d",
   "metadata": {
    "id": "f34ae84d"
   },
   "source": [
    "### Testando na YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e2bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30200929",
   "metadata": {
    "id": "30200929"
   },
   "outputs": [],
   "source": [
    "yaml_content = f\"\"\"\n",
    "train: images/train\n",
    "val: images/val\n",
    "\n",
    "nc: {len(label_dict)}\n",
    "names: {list(label_dict.keys())}\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(yolo_dataset_path, 'data.yaml'), 'w') as f:\n",
    "    f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79713165",
   "metadata": {
    "id": "79713165"
   },
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # Use the YOLOv8 nano model\n",
    "\n",
    "# Train the model\n",
    "model.train(\n",
    "    data=os.path.join(yolo_dataset_path, 'data.yaml'),  # Path to dataset config file\n",
    "    epochs=2,  # Number of epochs\n",
    "    imgsz=640,  # Image size\n",
    "    batch=16,  # Batch size\n",
    "    name='yolov8_test'  # Experiment name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8a2c0",
   "metadata": {
    "id": "a5f8a2c0"
   },
   "outputs": [],
   "source": [
    "results = model.predict(source=os.path.join(yolo_dataset_path, 'images/val/'), save=True)  # Run inference on validation set\n",
    "\n",
    "# Display predictions\n",
    "for result in results:\n",
    "    result.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5882cb8",
   "metadata": {
    "id": "f5882cb8"
   },
   "outputs": [],
   "source": [
    "max_conf = 0\n",
    "best_result = None\n",
    "\n",
    "for result in results:\n",
    "        if (result.boxes.conf.mean().item() > max_conf) and (result.boxes.conf.mean().item() < 0.98) and (result.boxes.conf.shape[0] > 2):\n",
    "            max_conf = result.boxes.conf.mean().item()\n",
    "            best_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d5a5ec",
   "metadata": {
    "id": "b8d5a5ec"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "img = best_result.orig_img\n",
    "boxes = best_result.boxes.xyxy.cpu().numpy()  # xyxy format\n",
    "classe = best_result.boxes.cls\n",
    "confianca = best_result.boxes.conf\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    x1, y1, x2, y2 = box[:4]\n",
    "    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    # Add class and confidence\n",
    "    class_idx = int(classe[i])\n",
    "    class_name = best_result.names[class_idx] if hasattr(best_result, 'names') else str(class_idx)\n",
    "    conf = float(confianca[i])\n",
    "    ax.text(x1, y1 - 5, f'{class_name}: {conf:.2f}', color='red', fontsize=10, backgroundcolor='white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e08478",
   "metadata": {
    "id": "83e08478"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7ffaf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T16:23:55.152554Z",
     "iopub.status.busy": "2025-09-20T16:23:55.152291Z",
     "iopub.status.idle": "2025-09-20T16:23:55.159142Z",
     "shell.execute_reply": "2025-09-20T16:23:55.158455Z",
     "shell.execute_reply.started": "2025-09-20T16:23:55.152534Z"
    },
    "id": "28a7ffaf",
    "outputId": "e03f4e94-a26c-49f6-81c4-1b86ab96bd0e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torchvision.io import decode_image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "#from torchtune.datasets import ConcatDataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "from typing import Literal\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fbb43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:52.243555Z",
     "iopub.status.busy": "2025-09-20T15:29:52.243064Z",
     "iopub.status.idle": "2025-09-20T15:29:52.247069Z",
     "shell.execute_reply": "2025-09-20T15:29:52.246276Z",
     "shell.execute_reply.started": "2025-09-20T15:29:52.243537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_DIM = 320\n",
    "YOLO_DATASET_PATH = \"yolo_dataset\"\n",
    "UNET_DATASET_PATH = \"unet_dataset\"\n",
    "MASK_DATASET_PATH = \"mask_dataset\"\n",
    "DEVICE = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "COLORMAP = plt.get_cmap(\"hsv\")\n",
    "\n",
    "print(f\"using device '{DEVICE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f505d",
   "metadata": {},
   "source": [
    "# Funções Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e45d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:52.825498Z",
     "iopub.status.busy": "2025-09-20T15:29:52.825226Z",
     "iopub.status.idle": "2025-09-20T15:29:52.832298Z",
     "shell.execute_reply": "2025-09-20T15:29:52.831583Z",
     "shell.execute_reply.started": "2025-09-20T15:29:52.825477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def iou_wh(box_1, box_2):\n",
    "    intersection = torch.min(box_1[0], box_2[:, 0]) * torch.min(box_1[1], box_2[:, 1])\n",
    "    union = (box_1[0] * box_1[1]) + (box_2[:, 0] * box_2[:, 1]) - intersection\n",
    "    return intersection / union\n",
    "\n",
    "def iou_xy(box1, box2):\n",
    "    b1_x1 = box1[1] - box1[3]/2\n",
    "    b1_x2 = box1[1] + box1[3]/2\n",
    "    b1_y1 = box1[2] - box1[4]/2\n",
    "    b1_y2 = box1[2] + box1[4]/2\n",
    "\n",
    "    b2_x1 = box2[..., 1] - box2[..., 3]/2\n",
    "    b2_x2 = box2[..., 1] + box2[..., 3]/2\n",
    "    b2_y1 = box2[..., 2] - box2[..., 4]/2\n",
    "    b2_y2 = box2[..., 2] + box2[..., 4]/2\n",
    "\n",
    "    x_overlap = torch.clamp(torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1), min = 0)\n",
    "    y_overlap = torch.clamp(torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1), min = 0)\n",
    "\n",
    "    intersection = x_overlap * y_overlap\n",
    "\n",
    "    union = (b1_x2 - b1_x1) * (b1_y2 - b1_y1) + (b2_x2 - b2_x1) * (b2_y2 - b2_y1) - intersection\n",
    "\n",
    "    return intersection / (union + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9dd346",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:53.013106Z",
     "iopub.status.busy": "2025-09-20T15:29:53.012839Z",
     "iopub.status.idle": "2025-09-20T15:29:53.019698Z",
     "shell.execute_reply": "2025-09-20T15:29:53.019056Z",
     "shell.execute_reply.started": "2025-09-20T15:29:53.013090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_yolo_images(dataset, index, title = None):\n",
    "    image = dataset.get_plot_image(index)\n",
    "    _, height, width = image.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu())\n",
    "    ax.set_title(title)\n",
    "    #ax.axis(\"off\")\n",
    "\n",
    "    boxes = dataset.get_plot_box(index)\n",
    "    for i, box in enumerate(boxes):\n",
    "        class_index = int(box[0])\n",
    "        class_name = dataset.classes[class_index]\n",
    "        conf = box[5]\n",
    "\n",
    "        cx, cy, w, h = box[1:5]\n",
    "        cx *= width\n",
    "        cy *= height\n",
    "        w *= width\n",
    "        h *= height\n",
    "        rect = patches.Rectangle(\n",
    "            (cx - (w/2), cy - (h/2)),\n",
    "            w, h,\n",
    "            linewidth = 3, edgecolor = 'r', facecolor = 'none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            cx - (w/2) + 4, cy - (h/2) - 8,\n",
    "            f\"{class_name}: {conf:.2f}\",\n",
    "            color = \"white\", fontsize = 9,\n",
    "            bbox=dict(facecolor=\"red\", alpha=0.8, pad=1.5, edgecolor=\"none\")\n",
    "        ).set_clip_on(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28dd1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:53.143011Z",
     "iopub.status.busy": "2025-09-20T15:29:53.142760Z",
     "iopub.status.idle": "2025-09-20T15:29:53.150133Z",
     "shell.execute_reply": "2025-09-20T15:29:53.149365Z",
     "shell.execute_reply.started": "2025-09-20T15:29:53.142994Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Pega o output e aplica as operações de tratamento da daísa\n",
    "def plot_yolo_output(model, dataset, index, title = None, confidence = 0.6, min_IoU = 0.4):\n",
    "    image = dataset.get_plot_image(index)\n",
    "    image, _ = dataset.letterbox_data(image, [])\n",
    "\n",
    "    _, height, width = image.shape\n",
    "\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu())\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model_out = model(dataset[index][0].unsqueeze(0))\n",
    "        all_boxes = dataset.decode_model_output(model_out)\n",
    "        filtered_boxes = dataset.non_maximum_supression(all_boxes, confidence, min_IoU)[0]\n",
    "        if filtered_boxes is None:\n",
    "            return\n",
    "        filtered_boxes = filtered_boxes.cpu()\n",
    "\n",
    "        for i, box in enumerate(filtered_boxes):\n",
    "            class_index = int(box[0])\n",
    "            class_name = dataset.classes[class_index]\n",
    "            conf = box[5]\n",
    "\n",
    "            cx, cy, w, h = box[1:5]\n",
    "            cx *= width\n",
    "            cy *= height\n",
    "            w *= width\n",
    "            h *= height\n",
    "\n",
    "            rect = patches.Rectangle(\n",
    "                (cx - (w/2), cy - (h/2)),\n",
    "                w, h,\n",
    "                linewidth = 3, edgecolor = COLORMAP(class_index/20), facecolor = \"none\",\n",
    "                alpha=1.0\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            ax.text(\n",
    "                cx - (w/2), cy - (h/2),\n",
    "                f\"{class_name}: {conf:.2f}\",\n",
    "                horizontalalignment=\"left\",\n",
    "                verticalalignment=\"bottom\",\n",
    "                color=\"white\", fontsize=10,\n",
    "                bbox=dict(facecolor=COLORMAP(class_index/20), alpha=1.0, pad=1.5, edgecolor=\"none\")\n",
    "            ).set_clip_on(True)\n",
    "\n",
    "            \n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce3958",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987ea97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:53.431928Z",
     "iopub.status.busy": "2025-09-20T15:29:53.431343Z",
     "iopub.status.idle": "2025-09-20T15:29:53.457733Z",
     "shell.execute_reply": "2025-09-20T15:29:53.456983Z",
     "shell.execute_reply.started": "2025-09-20T15:29:53.431902Z"
    },
    "id": "3987ea97",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class YoloDataset(Dataset):\n",
    "\n",
    "    def __init__(self, type_, dim = 416, dataset_path = [\"yolo_dataset\"], device = \"cpu\"):\n",
    "        self.base_path_data = os.path.join(*dataset_path, \"images\", f\"{type_}\")\n",
    "        self.base_path_labels = os.path.join(*dataset_path, \"labels\", f\"{type_}\")\n",
    "\n",
    "        self.images = glob(os.path.join(self.base_path_data, \"*.jpg\"))\n",
    "        self.load_boxes()\n",
    "\n",
    "        self.device = device\n",
    "        self.dim = dim\n",
    "\n",
    "        self.classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "        self.strides = [32, 16, 8]\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        mean = [115.7482, 111.8405, 105.9740]\n",
    "        std = [52.7395, 51.7852, 53.8093]\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def load_boxes(self):\n",
    "        self.boxes = []\n",
    "\n",
    "        for i, img_path in enumerate(self.images):\n",
    "            img_path = self.images[i]\n",
    "            name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "            label_path = os.path.join(self.base_path_labels, f\"{name}.txt\")\n",
    "\n",
    "            annotations = []\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    annotations.append([float(x) for x in line.split()])\n",
    "            \n",
    "            self.boxes.append(torch.tensor(annotations, dtype = torch.float32, requires_grad = False))\n",
    "\n",
    "    #Feito na inicialização da classe para pegar as prioris de caixas\n",
    "    def clusterize_anchors(self, n_anchors):\n",
    "        self.n_anchors = n_anchors\n",
    "\n",
    "        labels = glob(os.path.join(self.base_path_labels, \"*.txt\"))\n",
    "        lengths = torch.cat(self.boxes)[:, 3:5]\n",
    "\n",
    "        kmeans = KMeans(self.n_anchors, n_init=\"auto\")\n",
    "        kmeans.fit(lengths)\n",
    "\n",
    "        self.anchors = torch.tensor(kmeans.cluster_centers_, dtype = torch.float32).to(device)\n",
    "\n",
    "    def set_anchors(self, anchors):\n",
    "        self.anchors = anchors\n",
    "        self.n_anchors = len(anchors)\n",
    "\n",
    "    def get_anchors(self):\n",
    "        return self.anchors\n",
    "\n",
    "    def letterbox_data(self, image, image_boxes):\n",
    "        if image.shape[2] > image.shape[1]: #W > H\n",
    "            scale_factor = self.dim/image.shape[2]\n",
    "            new_H = int(scale_factor * image.shape[1])\n",
    "            new_dims = (new_H, self.dim)\n",
    "\n",
    "            resize = T.Resize(new_dims)\n",
    "            a = int(round((self.dim - new_H)/2 - 0.1))\n",
    "            b = int(round((self.dim - new_H)/2 + 0.1))\n",
    "            padding = T.Pad([0, a, 0, b], fill = 117)\n",
    "\n",
    "            box_scale = new_dims[0]/self.dim\n",
    "            for box in image_boxes:\n",
    "                box[2] = ((box[2] - 0.5) * box_scale) + 0.5\n",
    "                box[4] *= box_scale\n",
    "\n",
    "\n",
    "        elif image.shape[2] < image.shape[1]: #W < H\n",
    "            scale_factor = self.dim/image.shape[1]\n",
    "            new_W = int(scale_factor * image.shape[2])\n",
    "            new_dims = (self.dim, new_W)\n",
    "\n",
    "            resize = T.Resize(new_dims)\n",
    "            a = int(round((self.dim - new_W)/2 - 0.1))\n",
    "            b = int(round((self.dim - new_W)/2 + 0.1))\n",
    "            padding = T.Pad([a, 0, b, 0], fill = 117)\n",
    "            box_scale = new_dims[1]/self.dim\n",
    "            for box in image_boxes:\n",
    "                box[1] = ((box[1] - 0.5) * box_scale) + 0.5\n",
    "                box[3] *= box_scale\n",
    "\n",
    "        else: #W = H\n",
    "            resize = T.Resize((self.dim, self.dim))\n",
    "            padding = lambda x: x\n",
    "\n",
    "        return padding(resize(image)), image_boxes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = decode_image(img_path).to(torch.float32)\n",
    "        image = self.transform(image).to(self.device)\n",
    "        boxes = self.boxes[idx].clone()\n",
    "\n",
    "        #Dá letterbox na imagem e nas boxes\n",
    "        letterbox_image, boxes = self.letterbox_data(image, boxes)\n",
    "\n",
    "        #Agora com a imagem e boxes no tamanho certo, cria as boxes no formato do modelo\n",
    "        matrixes = [] #Vai ter as saídas pra cada stride\n",
    "        for stride in self.strides:\n",
    "            if self.dim % stride != 0:\n",
    "                raise ValueError(f\"Dimensão {self.dim} incompatível com o stride {stride}\")\n",
    "            n_cells = self.dim//stride\n",
    "            cell_relative_size = 1/n_cells\n",
    "\n",
    "            output_matrix = torch.zeros((self.n_anchors, n_cells, n_cells, 5 + self.n_classes), dtype = torch.float32).to(self.device)\n",
    "\n",
    "            for box in boxes:\n",
    "                cell_x, cell_y = int(box[1]/cell_relative_size), int(box[2]/cell_relative_size) #H depois W\n",
    "\n",
    "                dx = box[1] * n_cells - cell_x\n",
    "                dy = box[2] * n_cells - cell_y\n",
    "\n",
    "                IoUs = iou_wh(box[3:5], self.anchors)\n",
    "                assigned_anchor_index = IoUs.argmax()\n",
    "                if output_matrix[assigned_anchor_index, cell_y, cell_x, 0] == 1:\n",
    "                    continue\n",
    " #                   raise Exception(\"Duas bounding boxes anotadas ficaram associadas à mesma célula e a mesma anchor\")\n",
    "                pw = torch.log(box[3] / self.anchors[assigned_anchor_index][0] + 1e-5)\n",
    "                ph = torch.log(box[4] / self.anchors[assigned_anchor_index][1] + 1e-5)\n",
    "            \n",
    "                output_matrix[assigned_anchor_index, cell_y, cell_x, :5] = torch.tensor((1, dx, dy, pw, ph), dtype = torch.float32)\n",
    "                output_matrix[assigned_anchor_index, cell_y, cell_x, 5 + int(box[0])] = 1\n",
    "            matrixes.append(output_matrix)\n",
    "\n",
    "        return letterbox_image, matrixes\n",
    "\n",
    "    #Função para pegar as boxes formatadas pro MAp\n",
    "    def get_formated_boxes(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = decode_image(img_path).to(torch.float32).to(self.device)\n",
    "        boxes = self.boxes[idx]\n",
    "\n",
    "        #Dá letterbox na imagem e nas boxes\n",
    "        letterbox_image, boxes = self.letterbox_data(image, boxes)\n",
    "\n",
    "        dict_ = {\n",
    "            \"labels\": boxes[:, 0].clone().to(torch.uint64),\n",
    "            \"boxes\": boxes[:, 1:5].clone() * self.dim\n",
    "        }\n",
    "        return dict_\n",
    "\n",
    "    #Função auxiliar para pegar as boxes com confiança\n",
    "    def get_plot_box(self, idx):\n",
    "        boxes = self.boxes[idx]\n",
    "        ones = torch.ones(boxes.shape[0], 1)\n",
    "        boxes_with_confidence = torch.cat((boxes, ones), dim = 1)\n",
    "        return boxes_with_confidence\n",
    "\n",
    "    #Função auxiliar para pegar a imagem original sem normalização\n",
    "    def get_plot_image(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        return decode_image(img_path)\n",
    "\n",
    "    def non_maximum_supression(self, boxes, confidence = 0.5, min_IoU = 0.5): #Espera lista com previsões por stride, previsões tem 5 (a primeira é batch)\n",
    "        batch_boxes = []\n",
    "        for image_index in range(boxes.shape[0]): #itera por imagem do batch, n tem como fazer de uma vez\n",
    "\n",
    "            image_boxes = boxes[image_index]\n",
    "            image_boxes = image_boxes[image_boxes[:, 5] > confidence]\n",
    "            if image_boxes.shape[0] == 0:\n",
    "                batch_boxes.append(None)\n",
    "                continue\n",
    "\n",
    "            filtered_boxes = []\n",
    "            pred_classes = torch.unique(image_boxes[:, 0])\n",
    "            for class_ in pred_classes:\n",
    "                class_boxes = image_boxes[image_boxes[:, 0] == class_]\n",
    "                _, sort_ind = torch.sort(class_boxes[:, 5], descending = True)\n",
    "                class_boxes = class_boxes[sort_ind]\n",
    "                for i in range(class_boxes.shape[0]):\n",
    "                    try:\n",
    "                        IoUs = iou_xy(class_boxes[i], class_boxes[i+1:]) #Calcula IoU para caixas abaixo\n",
    "                    except Exception as e:\n",
    "                        break\n",
    "                    remove_mask = torch.ones(class_boxes.shape[0], dtype = bool).to(self.device)\n",
    "                    remove_mask[i+1:] = IoUs < min_IoU\n",
    "                    class_boxes = class_boxes[remove_mask, :]\n",
    "                filtered_boxes.append(class_boxes)\n",
    "            image_boxes = torch.cat(filtered_boxes)\n",
    "            batch_boxes.append(image_boxes)\n",
    "        return batch_boxes\n",
    "\n",
    "    #Recebe a saída bruta do modelo e retorna um tensor de bounding boxes no formato B x N x [classe, cx, cy, w, h, confiança]\n",
    "    def decode_model_output(self, output):\n",
    "        reshaped_tensors = []\n",
    "        for stride, matrix in zip(self.strides, output):\n",
    "            matrix = matrix.detach().clone()\n",
    "            #Matrix aqui: B x A x N_C x N_C x (5 + C)\n",
    "            batch_size, n_anchors = int(matrix.shape[0]), int(matrix.shape[1])\n",
    "            n_cells = self.dim // stride\n",
    "            bounding_boxes_len = int(matrix.shape[4])\n",
    "\n",
    "            #Fazendo as transformações de sigmoide na confiança e nas coordenadas de centro\n",
    "            matrix[..., 0:3] = self.sigmoid(matrix[..., 0:3])\n",
    "\n",
    "            #Fazendo as transformações de sigmoide nos scores de classe (não sei se precisa)\n",
    "            matrix[..., 5:] = self.sigmoid(matrix[..., 5:])\n",
    "\n",
    "            #Fazendo as transformações para obter o tamanho real da bounding box\n",
    "            #TODO: tirar esse for\n",
    "            for i in range(self.anchors.shape[0]):\n",
    "                matrix[:, i, ..., 3:5] = self.anchors[i] * torch.exp(matrix[:, i, ..., 3:5])\n",
    "\n",
    "            grid_y, grid_x = torch.meshgrid(torch.arange(n_cells), torch.arange(n_cells), indexing=\"ij\")\n",
    "            matrix[:, :, :, :, 1] += grid_x.to(matrix.device)\n",
    "            matrix[:, :, :, :, 2] += grid_y.to(matrix.device)\n",
    "            matrix[..., 1:3] /= n_cells\n",
    "\n",
    "            #Agora não dependo mais de saber qual priori de ancora era e nem a célula, já incorporei essas infos na caixa\n",
    "            matrix = matrix.reshape((batch_size, n_anchors * n_cells**2, bounding_boxes_len)) #batch size - lista de bounding boxes - bounding boxes\n",
    "            #TODO: melhorar isso\n",
    "            reshaped_tensors.append(matrix)\n",
    "            classes = matrix[:, :, 5:].argmax(-1)\n",
    "            matrix = matrix[:, :, :6]\n",
    "            matrix[:, :, 5] = matrix[:, :, 0]\n",
    "            matrix[:, :, 0] = classes\n",
    "        return torch.cat(reshaped_tensors, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11e8f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:53.552005Z",
     "iopub.status.busy": "2025-09-20T15:29:53.551278Z",
     "iopub.status.idle": "2025-09-20T15:29:54.139747Z",
     "shell.execute_reply": "2025-09-20T15:29:54.139174Z",
     "shell.execute_reply.started": "2025-09-20T15:29:53.551982Z"
    },
    "id": "ac11e8f0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_y = YoloDataset(\"train\", IMG_DIM, [f\"{YOLO_DATASET_PATH}\"], device)\n",
    "train_data_y.clusterize_anchors(3)\n",
    "anchors = train_data_y.get_anchors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbc0d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:54.141384Z",
     "iopub.status.busy": "2025-09-20T15:29:54.141124Z",
     "iopub.status.idle": "2025-09-20T15:29:54.278060Z",
     "shell.execute_reply": "2025-09-20T15:29:54.277471Z",
     "shell.execute_reply.started": "2025-09-20T15:29:54.141364Z"
    },
    "id": "6bbc0d96",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "val_data_y = YoloDataset(\"val\", IMG_DIM, [f\"{YOLO_DATASET_PATH}\"], device)\n",
    "val_data_y.set_anchors(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f28be3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:54.278889Z",
     "iopub.status.busy": "2025-09-20T15:29:54.278675Z",
     "iopub.status.idle": "2025-09-20T15:29:54.808589Z",
     "shell.execute_reply": "2025-09-20T15:29:54.807812Z",
     "shell.execute_reply.started": "2025-09-20T15:29:54.278873Z"
    },
    "id": "a3f28be3",
    "outputId": "a68aaf82-438a-4f60-fc99-47b592fd765a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plot_yolo_images(train_data_y, 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97918262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T15:29:54.810098Z",
     "iopub.status.busy": "2025-09-20T15:29:54.809880Z",
     "iopub.status.idle": "2025-09-20T15:29:54.814454Z",
     "shell.execute_reply": "2025-09-20T15:29:54.813615Z",
     "shell.execute_reply.started": "2025-09-20T15:29:54.810081Z"
    },
    "id": "97918262",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#--------------Data loader, erro, otimizador, etc----------------------\n",
    "\n",
    "train_dataloader_y = DataLoader(train_data_y, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_dataloader_y = DataLoader(val_data_y, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece06d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script true\n",
    "def data_mean_std(dataloader):\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    n_images = 0\n",
    "    \n",
    "    for images, _ in tqdm(dataloader):\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        n_images += batch_samples\n",
    "\n",
    "    mean /= n_images\n",
    "    std /= n_images\n",
    "    \n",
    "    return {\"Mean\":mean, \"Std\":std}\n",
    "\n",
    "data_mean_std(train_dataloader_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683325d3",
   "metadata": {
    "id": "683325d3"
   },
   "source": [
    "# Função de perda YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094f2496",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:36.276540Z",
     "iopub.status.busy": "2025-09-20T16:48:36.276268Z",
     "iopub.status.idle": "2025-09-20T16:48:36.288523Z",
     "shell.execute_reply": "2025-09-20T16:48:36.287658Z",
     "shell.execute_reply.started": "2025-09-20T16:48:36.276522Z"
    },
    "id": "094f2496",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class YOLOv3Loss(nn.Module):\n",
    "    # (N, B, (tx,ty,tw,th,t0), S, S)\n",
    "\n",
    "    # pred, true tensor\n",
    "    # (N, B, S, S, 5+C)\n",
    "    # N: batch size\n",
    "    # B: n anchors\n",
    "    # SxS: grid size\n",
    "    # 5+C [p0, x, y, w, h, p1,...,pc]\n",
    "\n",
    "    def __init__(self, anchors):\n",
    "        super().__init__()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "        self.lambda_class = 1\n",
    "        self.lambda_obj = 1\n",
    "\n",
    "        self.anchors = anchors\n",
    "\n",
    "    def compute_noobj_loss(self, pred, true, noobj_mask):\n",
    "        loss = self.bce(\n",
    "            pred[..., 0:1][noobj_mask],\n",
    "            true[..., 0:1][noobj_mask],\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_obj_loss(self, pred, true, obj_mask):\n",
    "        loss = self.bce(\n",
    "            pred[..., 0:1][obj_mask],\n",
    "            true[..., 0:1][obj_mask]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def compute_coord_loss(self, pred, true, obj_mask):\n",
    "        box_pred = pred[..., 1:5][obj_mask]\n",
    "        box_true = true[..., 1:5][obj_mask]\n",
    "\n",
    "        wh_pred = box_pred[..., 2:4]\n",
    "        wh_true = box_true[..., 2:4]\n",
    "\n",
    "        N, B, S, _ = true.shape[:4]\n",
    "\n",
    "        anchors_new= self.anchors.reshape(1, B, 1, 1, 2).expand(N, B, S, S, 2).to(true.device)\n",
    "        anchor_priors = anchors_new[obj_mask]\n",
    "\n",
    "        loss_xy = self.mse(\n",
    "            self.sigmoid(box_pred[..., :2]), box_true[..., :2]\n",
    "        )\n",
    "\n",
    "        loss_wh = self.mse(\n",
    "            # wh_pred, wh_true/anchor_priors\n",
    "            wh_pred, wh_true\n",
    "        )\n",
    "        return loss_xy + loss_wh\n",
    "\n",
    "\n",
    "    def compute_class_loss(self, pred, true, obj_mask):\n",
    "        #print(pred.shape)\n",
    "        #print(true.shape)\n",
    "        #print(obj_mask.shape)\n",
    "        obj_pred = pred[obj_mask]\n",
    "        obj_true = true[obj_mask]\n",
    "        #print(obj_pred.shape)\n",
    "        #print(obj_true.shape)\n",
    "\n",
    "        pred_class = obj_pred[..., 5:]\n",
    "        #print(pred_class.shape)\n",
    "        true_class = obj_true[..., 5:]\n",
    "        #print(true_class.shape)\n",
    "\n",
    "        #true_class = torch.zeros_like(pred_class)\n",
    "        #true_class.scatter_(1, true_indices.unsqueeze(1), 1)\n",
    "\n",
    "        loss = self.bce(pred_class, true_class)\n",
    "\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        final_loss = 0\n",
    "        for i in range(len(pred)):\n",
    "            #print(true[i].shape, pred[i].shape)\n",
    "            obj_mask = true[i][..., 0] == 1\n",
    "            noobj_mask = ~obj_mask\n",
    "\n",
    "            obj_loss = self.compute_obj_loss(pred[i], true[i], obj_mask)\n",
    "            noobj_loss = self.compute_noobj_loss(pred[i], true[i], noobj_mask)\n",
    "            coord_loss = self.compute_coord_loss(pred[i], true[i], obj_mask)\n",
    "            class_loss = self.compute_class_loss(pred[i], true[i], obj_mask)\n",
    "\n",
    "            final_loss += self.lambda_noobj * noobj_loss + \\\n",
    "                          self.lambda_coord * coord_loss + \\\n",
    "                          obj_loss + \\\n",
    "                          class_loss\n",
    "        return final_loss.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05055b6a",
   "metadata": {
    "id": "05055b6a"
   },
   "source": [
    "# Classe de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f21f8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:36.827795Z",
     "iopub.status.busy": "2025-09-20T16:48:36.827485Z",
     "iopub.status.idle": "2025-09-20T16:48:36.840445Z",
     "shell.execute_reply": "2025-09-20T16:48:36.839742Z",
     "shell.execute_reply.started": "2025-09-20T16:48:36.827774Z"
    },
    "id": "98f21f8f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, criterion, plot_func):\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.history = {\n",
    "            \"epochs\": 0,\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": []\n",
    "        }\n",
    "\n",
    "        self.plot_pred = plot_func\n",
    "\n",
    "    def train_one_epoch(self, optimizer, train_loader, epoch):\n",
    "        self.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc = f\"Epoch {epoch + 1} | Training\")\n",
    "        for i, (X, y) in enumerate(progress_bar):\n",
    "            pred = self(X)\n",
    "            loss = self.criterion(pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * X.size(0)\n",
    "\n",
    "            progress_bar.set_postfix(loss = loss.item())\n",
    "            #if i % 300 == 0:\n",
    "            #    plot_model_output(self, train_loader.dataset, 0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        progress_bar.set_postfix(loss = epoch_loss)\n",
    "        return epoch_loss\n",
    "\n",
    "    def validate_one_epoch(self, optimizer, val_loader, epoch):\n",
    "        self.eval()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(val_loader, desc = f\"Epoch {epoch + 1} | Validating\")\n",
    "        with torch.no_grad():\n",
    "            for X, y in progress_bar:\n",
    "\n",
    "                pred = self(X)\n",
    "                loss = self.criterion(pred, y)\n",
    "\n",
    "                running_loss += loss.item() * X.size(0)\n",
    "\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(val_loader.dataset)\n",
    "        progress_bar.set_postfix(loss = epoch_loss)\n",
    "\n",
    "        self.plot_pred(self, val_loader.dataset, 1)\n",
    "        \n",
    "        return epoch_loss\n",
    "\n",
    "    def save_model(self, path = None):\n",
    "        if path:\n",
    "            json_path = os.path.join(*path, f\"{self.__class__.__name__}.json\")    \n",
    "            pth_path = os.path.join(*path, f\"{self.__class__.__name__}.pth\")\n",
    "        else:\n",
    "            json_path = f\"{self.__class__.__name__}.json\"\n",
    "            pth_path = f\"{self.__class__.__name__}.pth\"\n",
    "        with open(json_path, \"w\", encoding = \"utf-8\") as f:\n",
    "            json.dump(self.history, f, ensure_ascii = False, indent = 4)\n",
    "        torch.save(self.state_dict(), pth_path)\n",
    "    \n",
    "    def load_model(self, path = None):\n",
    "        if path:\n",
    "            json_path = os.path.join(*path, f\"{self.__class__.__name__}.json\")    \n",
    "            pth_path = os.path.join(*path, f\"{self.__class__.__name__}.pth\")\n",
    "        else:\n",
    "            json_path = f\"{self.__class__.__name__}.json\"\n",
    "            pth_path = f\"{self.__class__.__name__}.pth\"\n",
    "        with open(json_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            self.history = json.load(f)\n",
    "        self.load_state_dict(torch.load(pth_path, map_location=torch.device(DEVICE), weights_only = True))\n",
    "\n",
    "    def plot_hist(self):\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        fig.suptitle(f\"Histórico de treinamento - {self.__name__}\")\n",
    "\n",
    "        axes[0].plot(hist[\"train_loss\"], label = \"Train\")\n",
    "        axes[0].plot(hist[\"val_loss\"], label = \"Validation\")\n",
    "        axes[0].set_xlabel(\"Epoch\")\n",
    "        axes[0].set_ylabel(\"Loss de treino\")\n",
    "        axes[0].set_title(\"Perda no treino\")\n",
    "        axes[0].legend()\n",
    "\n",
    "    def fit(self, train_loader, val_loader, optimizer, num_epochs, scheduler = None):\n",
    "        for epoch in range(self.history[\"epochs\"], self.history[\"epochs\"] + num_epochs):\n",
    "            train_loss = self.train_one_epoch(optimizer, train_loader, epoch)\n",
    "            val_loss = self.validate_one_epoch(optimizer, val_loader, epoch)\n",
    "\n",
    "            # print(f\"Fim Epoch {epoch+1}:\")\n",
    "            # print(f\"   -> Loss train: {train_loss:.3f} | Loss val: {val_loss:.3f}\")\n",
    "            if scheduler:\n",
    "                scheduler.step(val_loss)\n",
    "                print(f\"   -> LR: {scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "\n",
    "            self.history[\"epochs\"] = epoch\n",
    "            self.history[\"train_loss\"].append(train_loss)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                self.save_model()\n",
    "\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f6fec",
   "metadata": {
    "id": "bf0f6fec"
   },
   "source": [
    "# Arquitetura YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a911c4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:36.940324Z",
     "iopub.status.busy": "2025-09-20T16:48:36.940159Z",
     "iopub.status.idle": "2025-09-20T16:48:36.947951Z",
     "shell.execute_reply": "2025-09-20T16:48:36.947237Z",
     "shell.execute_reply.started": "2025-09-20T16:48:36.940311Z"
    },
    "id": "3a911c4b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConvBlockSpec:\n",
    "    out_channels: int\n",
    "    kernel_size: int\n",
    "    stride: int\n",
    "    padding: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.padding = 1 if self.kernel_size == 3 else 0\n",
    "\n",
    "@dataclass\n",
    "class ResidualBlockSpec:\n",
    "    num_repeats: int\n",
    "    channels: int\n",
    "\n",
    "@dataclass\n",
    "class ScaleBlockSpec:\n",
    "    from_layer: int = -1\n",
    "\n",
    "@dataclass\n",
    "class UpsampleBlockSpec:\n",
    "    from_layer: int = -1\n",
    "\n",
    "config = [\n",
    "    #===DARKNET-53===\n",
    "    ConvBlockSpec(32, 3, 1),\n",
    "    ConvBlockSpec(64, 3, 2),\n",
    "    ResidualBlockSpec(1, 64),\n",
    "\n",
    "    ConvBlockSpec(128, 3, 2),\n",
    "    ResidualBlockSpec(2, 128),\n",
    "\n",
    "    ConvBlockSpec(256, 3, 2),\n",
    "    ResidualBlockSpec(8, 256),\n",
    "\n",
    "    ConvBlockSpec(512, 3, 2),\n",
    "    ResidualBlockSpec(8, 512),\n",
    "\n",
    "    ConvBlockSpec(1024, 3, 2),\n",
    "    ResidualBlockSpec(4, 1024),\n",
    "    #===DARKNET-53===\n",
    "\n",
    "    ConvBlockSpec(512, 1, 1),\n",
    "    ConvBlockSpec(1024, 1, 1),\n",
    "    ScaleBlockSpec(),\n",
    "\n",
    "    ConvBlockSpec(256, 1, 1),\n",
    "    UpsampleBlockSpec(),\n",
    "\n",
    "    ConvBlockSpec(256, 1, 1),\n",
    "    ConvBlockSpec(512, 1, 1),\n",
    "    ScaleBlockSpec(),\n",
    "\n",
    "    ConvBlockSpec(128, 1, 1),\n",
    "    UpsampleBlockSpec(),\n",
    "\n",
    "    ConvBlockSpec(128, 1, 1),\n",
    "    ConvBlockSpec(256, 1, 1),\n",
    "    ScaleBlockSpec(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c656a1c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:37.002746Z",
     "iopub.status.busy": "2025-09-20T16:48:37.002560Z",
     "iopub.status.idle": "2025-09-20T16:48:37.014947Z",
     "shell.execute_reply": "2025-09-20T16:48:37.014293Z",
     "shell.execute_reply.started": "2025-09-20T16:48:37.002733Z"
    },
    "id": "c656a1c2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        use_bias = not bn_act\n",
    "        layers = []\n",
    "\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, out_channels, bias=use_bias, **kwargs)\n",
    "        )\n",
    "\n",
    "        if bn_act:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residuals=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_residuals = use_residuals\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        for _ in range(num_repeats):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels//2, kernel_size=1),\n",
    "                    CNNBlock(channels//2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.block:\n",
    "            if self.use_residuals:\n",
    "                #print(x.shape)\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScaleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, num_anchors=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2*in_channels,\n",
    "                self.num_anchors *(num_classes+5),\n",
    "                bn_act=False,\n",
    "                kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # [p, cx, cy, w, h, Ic1, ..., Icn] * num_anchors\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.pred(x)\n",
    "\n",
    "        # N, C, W, H\n",
    "        N, _, S, _ = pred.shape\n",
    "\n",
    "        pred = pred.reshape(N, self.num_anchors, self.num_classes+5, S, S)\n",
    "        pred = pred.permute(0, 1, 3, 4, 2)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module, Trainer):\n",
    "    def __init__(self, config, criterion, plot_func,\n",
    "        num_anchors = 5, in_channels = 3, num_classes = 20\n",
    "    ):\n",
    "        super().__init__()\n",
    "        Trainer.__init__(self, criterion, plot_func)\n",
    "        self.config = config\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.num_anchors = num_anchors\n",
    "\n",
    "        self.layers = self.gen_layers()\n",
    "\n",
    "    def gen_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for layer in self.config:\n",
    "            if isinstance(layer, ConvBlockSpec):\n",
    "                layers.append(CNNBlock(in_channels, **asdict(layer)))\n",
    "                in_channels = layer.out_channels\n",
    "            elif isinstance(layer, ResidualBlockSpec):\n",
    "                layers.append(ResidualBlock(**asdict(layer)))\n",
    "            elif isinstance(layer, ScaleBlockSpec):\n",
    "                layers += [\n",
    "                    ResidualBlock(in_channels, use_residuals=False, num_repeats=1),\n",
    "                    CNNBlock(in_channels, in_channels//2, kernel_size=1),\n",
    "                    ScaleBlock(in_channels//2, self.num_classes, num_anchors=self.num_anchors)\n",
    "                ]\n",
    "                in_channels = in_channels//2\n",
    "\n",
    "            elif isinstance(layer, UpsampleBlockSpec):\n",
    "                layers.append(nn.Upsample(scale_factor=2))\n",
    "                in_channels = in_channels*3\n",
    "\n",
    "        return layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        preds = []\n",
    "        route_connections = []\n",
    "\n",
    "        #print(self.layers)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # print(isinstance(layer, CNNBlock))\n",
    "            # print(isinstance(layer, ResidualBlock))\n",
    "            # print(isinstance(layer, ScaleBlock))\n",
    "            # print()\n",
    "\n",
    "            if isinstance(layer, ScaleBlock):\n",
    "#                print(\"scale\")\n",
    "                preds.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            if isinstance(layer, nn.Upsample):\n",
    "#                print(\"concat\")\n",
    "                x = torch.cat([x, route_connections.pop()], dim=1)\n",
    "\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eece39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:37.061078Z",
     "iopub.status.busy": "2025-09-20T16:48:37.060886Z",
     "iopub.status.idle": "2025-09-20T16:48:37.547004Z",
     "shell.execute_reply": "2025-09-20T16:48:37.546425Z",
     "shell.execute_reply.started": "2025-09-20T16:48:37.061063Z"
    },
    "id": "e2eece39",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "yolo = YOLOv3(\n",
    "    config, \n",
    "    YOLOv3Loss(train_data_y.get_anchors()),\n",
    "    plot_yolo_output,\n",
    "    num_anchors = 3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ef7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17828ac1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:37.549089Z",
     "iopub.status.busy": "2025-09-20T16:48:37.548894Z",
     "iopub.status.idle": "2025-09-20T16:48:37.552835Z",
     "shell.execute_reply": "2025-09-20T16:48:37.552252Z",
     "shell.execute_reply.started": "2025-09-20T16:48:37.549073Z"
    },
    "id": "17828ac1",
    "outputId": "c1c5ddd4-0c42-4484-934a-cb5335211f5b",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#   for x, y in train_dataloader:\n",
    "#       out = yolo(x)\n",
    "#   #    print(type(out))\n",
    "#       boxes = train_data.decode_model_output(out)\n",
    "#       for box in out:\n",
    "#           #print(box[:, :, :, :, 5:])\n",
    "#           argmaxes = box[..., 5:].argmax(dim = -1)\n",
    "#           classes, counts = torch.unique(argmaxes, return_counts = True)\n",
    "#           print(classes, counts, counts.sum())\n",
    "# #      print(boxes.shape)\n",
    "# #      boxes = train_data.non_maximum_supression(boxes, 0.5)\n",
    "# #      for box_batch in boxes:\n",
    "# #          print(box_batch.shape)\n",
    "#       break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7012b",
   "metadata": {},
   "source": [
    "# Teste output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IqDL04y8nJtW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:37.553602Z",
     "iopub.status.busy": "2025-09-20T16:48:37.553396Z",
     "iopub.status.idle": "2025-09-20T16:48:39.111089Z",
     "shell.execute_reply": "2025-09-20T16:48:39.110395Z",
     "shell.execute_reply.started": "2025-09-20T16:48:37.553585Z"
    },
    "id": "IqDL04y8nJtW",
    "outputId": "a974a0ae-0c30-46fa-88ed-e123f275ff94",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    plot_yolo_output(yolo, val_data_y, i, confidence = 0.8, min_IoU=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c00de6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.112767Z",
     "iopub.status.busy": "2025-09-20T16:48:39.112416Z",
     "iopub.status.idle": "2025-09-20T16:48:39.115819Z",
     "shell.execute_reply": "2025-09-20T16:48:39.115287Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.112742Z"
    },
    "id": "8c00de6f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#x = torch.randn((2, 3, IMG_DIM, IMG_DIM)).to(device)\n",
    "#print(x.dtype)\n",
    "#out = yolo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee410a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.116594Z",
     "iopub.status.busy": "2025-09-20T16:48:39.116405Z",
     "iopub.status.idle": "2025-09-20T16:48:39.131043Z",
     "shell.execute_reply": "2025-09-20T16:48:39.130387Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.116571Z"
    },
    "id": "cee410a0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for output in out:\n",
    "#     print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e35f9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.148352Z",
     "iopub.status.busy": "2025-09-20T16:48:39.148188Z",
     "iopub.status.idle": "2025-09-20T16:48:39.162133Z",
     "shell.execute_reply": "2025-09-20T16:48:39.161610Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.148339Z"
    },
    "id": "28e35f9a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "\n",
    "# for img in out[1]:\n",
    "#     print(img.shape)\n",
    "#     counter += 1\n",
    "\n",
    "# print(counter)\n",
    "#     #print(out[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d1507",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.162973Z",
     "iopub.status.busy": "2025-09-20T16:48:39.162746Z",
     "iopub.status.idle": "2025-09-20T16:48:39.175605Z",
     "shell.execute_reply": "2025-09-20T16:48:39.175044Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.162952Z"
    },
    "id": "ca9d1507",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# len(out[1][0,0,0,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ff581",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.176582Z",
     "iopub.status.busy": "2025-09-20T16:48:39.176348Z",
     "iopub.status.idle": "2025-09-20T16:48:39.191143Z",
     "shell.execute_reply": "2025-09-20T16:48:39.190528Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.176562Z"
    },
    "id": "523ff581",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# res = ResidualBlock(channels=32)\n",
    "# x = torch.randn((2, 32, IMG_DIM, IMG_DIM))\n",
    "# res(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d6e82",
   "metadata": {
    "id": "9b3d6e82"
   },
   "source": [
    "# mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f8198",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.193341Z",
     "iopub.status.busy": "2025-09-20T16:48:39.193173Z",
     "iopub.status.idle": "2025-09-20T16:48:39.209165Z",
     "shell.execute_reply": "2025-09-20T16:48:39.208569Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.193329Z"
    },
    "id": "a46f8198",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    b1_x1 = box1[0] - box1[2]/2\n",
    "    b1_x2 = box1[0] + box1[2]/2\n",
    "    b1_y1 = box1[1] - box1[3]/2\n",
    "    b1_y2 = box1[1] + box1[3]/2\n",
    "\n",
    "    b2_x1 = box2[..., 0] - box2[..., 2]/2\n",
    "    b2_x2 = box2[..., 0] + box2[..., 2]/2\n",
    "    b2_y1 = box2[..., 1] - box2[..., 3]/2\n",
    "    b2_y2 = box2[..., 1] + box2[..., 3]/2\n",
    "\n",
    "    x_overlap = torch.clamp(torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1), min = 0)\n",
    "    y_overlap = torch.clamp(torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1), min = 0)\n",
    "\n",
    "    intersection = x_overlap * y_overlap\n",
    "\n",
    "    box1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)\n",
    "    box2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)\n",
    "\n",
    "    union = box1_area + box2_area - intersection\n",
    "\n",
    "    return intersection / (union + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce704f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.209986Z",
     "iopub.status.busy": "2025-09-20T16:48:39.209761Z",
     "iopub.status.idle": "2025-09-20T16:48:39.227931Z",
     "shell.execute_reply": "2025-09-20T16:48:39.227388Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.209965Z"
    },
    "id": "7bce704f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def iou(box_a, boxes_b):\n",
    "    \"\"\"Calcula o IoU de uma caixa (box_a) com múltiplas caixas (boxes_b).\"\"\"\n",
    "    # Calcula a área da interseção\n",
    "    xy_max = torch.min(box_a[:, 2:], boxes_b[:, 2:])\n",
    "    xy_min = torch.max(box_a[:, :2], boxes_b[:, :2])\n",
    "    inter_dims = torch.clamp(xy_max - xy_min, min=0)\n",
    "    inter_area = inter_dims[:, 0] * inter_dims[:, 1]\n",
    "\n",
    "    # Calcula a área das caixas\n",
    "    area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])\n",
    "    area_b = (boxes_b[:, 2] - boxes_b[:, 0]) * (boxes_b[:, 3] - boxes_b[:, 1])\n",
    "\n",
    "    union_area = area_a + area_b - inter_area\n",
    "\n",
    "    return inter_area / torch.clamp(union_area, min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514dc47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.229155Z",
     "iopub.status.busy": "2025-09-20T16:48:39.228830Z",
     "iopub.status.idle": "2025-09-20T16:48:39.245419Z",
     "shell.execute_reply": "2025-09-20T16:48:39.244763Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.229133Z"
    },
    "id": "1514dc47",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mean_average_precision(pred, true, num_classes, iou_threshold=0.5):\n",
    "    average_precisions = []\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        detections = []\n",
    "        ground_truths = []\n",
    "\n",
    "        # [idx, p0, cx, cy, w, h]\n",
    "        for img_idx, p in enumerate(pred):\n",
    "            class_mask = p[\"labels\"] == c\n",
    "            if torch.any(class_mask):\n",
    "                boxes = p[\"boxes\"][class_mask]\n",
    "                scores = p[\"scores\"][class_mask]\n",
    "                for box, score in zip(boxes, scores):\n",
    "                    detections.append([img_idx, score.item(), *box.tolist()])\n",
    "\n",
    "        for img_idx, t in enumerate(true):\n",
    "            class_mask = t[\"labels\"] == c\n",
    "            if torch.any(class_mask):\n",
    "                boxes = t[\"boxes\"][class_mask]\n",
    "                for box in boxes:\n",
    "                    ground_truths.append([img_idx, *box.tolist()])\n",
    "\n",
    "        if not ground_truths:\n",
    "            continue\n",
    "\n",
    "\n",
    "        amount_bboxes = {}\n",
    "        for gt in ground_truths:\n",
    "            img_idx = gt[0]\n",
    "            amount_bboxes[img_idx] = amount_bboxes.get(img_idx, 0) + 1\n",
    "\n",
    "        for key, val in amount_bboxes.items():\n",
    "            amount_bboxes[key] = torch.zeros(val)\n",
    "\n",
    "        detections.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        TP = torch.zeros(len(detections))\n",
    "        FP = torch.zeros(len(detections))\n",
    "\n",
    "        total_true_bboxes = len(ground_truths)\n",
    "\n",
    "        t1 = time.time()\n",
    "        for det_idx, detection in enumerate(detections):\n",
    "            img_idx = detection[0]\n",
    "\n",
    "            gt_img = torch.tensor([gt[1:] for gt in ground_truths if gt[0] == img_idx])\n",
    "\n",
    "            if len(gt_img) == 0:\n",
    "                FP[det_idx] = 1\n",
    "                continue\n",
    "\n",
    "            ious = iou(torch.tensor(detection[2:]), gt_img)\n",
    "            best_iou, best_gt_idx = torch.max(ious, dim=0)\n",
    "\n",
    "            if best_iou > iou_threshold:\n",
    "                if amount_bboxes[img_idx][best_gt_idx] == 0:\n",
    "                    TP[det_idx] = 1\n",
    "                    amount_bboxes[img_idx][best_gt_idx] = 1 # Não pode reusar\n",
    "                else:\n",
    "                    FP[det_idx] = 1\n",
    "            else:\n",
    "                FP[det_idx] = 1\n",
    "        print(f\"tempo {time.time()-t1 :.2f}s\")\n",
    "\n",
    "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
    "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
    "\n",
    "        recalls = TP_cumsum / (total_true_bboxes + 1e-10)\n",
    "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + 1e-10)\n",
    "\n",
    "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
    "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
    "\n",
    "        ap = torch.trapz(precisions, recalls)\n",
    "        average_precisions.append(ap.item())\n",
    "\n",
    "    mean_ap = sum(average_precisions) / len(average_precisions) if average_precisions else 0\n",
    "    return mean_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sF2Oju3QLCjG",
   "metadata": {
    "id": "sF2Oju3QLCjG"
   },
   "source": [
    "# Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jDuaelad4vTh",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T16:48:39.246422Z",
     "iopub.status.busy": "2025-09-20T16:48:39.246228Z",
     "iopub.status.idle": "2025-09-20T16:48:39.265957Z",
     "shell.execute_reply": "2025-09-20T16:48:39.265320Z",
     "shell.execute_reply.started": "2025-09-20T16:48:39.246401Z"
    },
    "id": "jDuaelad4vTh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_map_format(nms_output, dim):\n",
    "    formatted_predictions = []\n",
    "\n",
    "    for batch_boxes in nms_output:\n",
    "        if batch_boxes is None or batch_boxes.shape[0] == 0:\n",
    "            formatted_predictions.append({\n",
    "                \"boxes\": torch.empty((0, 4), dtype=torch.float32),\n",
    "                \"labels\": torch.empty(0, dtype=torch.int64),\n",
    "                \"scores\": torch.empty(0, dtype=torch.float32),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        labels = batch_boxes[:, 0].to(torch.int64)\n",
    "        scores = batch_boxes[:, 5]\n",
    "        boxes_cxcywh = batch_boxes[:, 1:5]\n",
    "\n",
    "        formatted_predictions.append({\n",
    "            \"boxes\": boxes_cxcywh,\n",
    "            \"labels\": labels,\n",
    "            \"scores\": scores,\n",
    "        })\n",
    "\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8534761",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T17:47:16.939575Z",
     "iopub.status.busy": "2025-09-20T17:47:16.939300Z"
    },
    "id": "b8534761",
    "outputId": "1a0d04b8-e8c9-43a3-a426-661e15146e40",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(yolo.parameters(), lr = 0.0001)\n",
    "yolo.fit(train_dataloader_y, val_dataloader_y, optimizer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DQ2tmTIi5ElM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T17:41:57.046569Z",
     "iopub.status.busy": "2025-09-20T17:41:57.046354Z",
     "iopub.status.idle": "2025-09-20T17:41:57.405985Z",
     "shell.execute_reply": "2025-09-20T17:41:57.405225Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.046553Z"
    },
    "id": "DQ2tmTIi5ElM",
    "outputId": "44d3b865-22b0-43e9-c853-9b044b730ff9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = val_dataloader_y.dataset\n",
    "\n",
    "for batch in val_dataloader_y:\n",
    "\n",
    "    print(val_data_y.get_formated_boxes(idx = 0))\n",
    "\n",
    "    break\n",
    "    x, y = batch\n",
    "    pred = yolo(x)\n",
    "    decoded_boxes = dataset.decode_model_output(pred, dim=IMG_DIM)\n",
    "    nms_boxes = dataset.non_maximum_supression(decoded_boxes, confidence=0.25, min_IoU=0.45)\n",
    "    pred_for_map = convert_to_map_format(nms_boxes, dim=IMG_DIM)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9kF-Sc734zYR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-09-20T17:41:57.407173Z",
     "iopub.status.busy": "2025-09-20T17:41:57.406820Z",
     "iopub.status.idle": "2025-09-20T17:41:57.420010Z",
     "shell.execute_reply": "2025-09-20T17:41:57.419153Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.407148Z"
    },
    "id": "9kF-Sc734zYR",
    "outputId": "22a0d56c-5f44-4539-98e6-bc6c5890c18e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(pred_for_map[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j8DVvMvqgZkD",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-20T17:41:57.420428Z",
     "iopub.status.idle": "2025-09-20T17:41:57.420650Z",
     "shell.execute_reply": "2025-09-20T17:41:57.420556Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.420546Z"
    },
    "id": "j8DVvMvqgZkD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    0,0 ------> x (width)\n",
    "     |\n",
    "     | (Left,Top)\n",
    "     |      *_________\n",
    "     |      |         |\n",
    "            |         |\n",
    "     y      |_________|\n",
    "  (height)            *\n",
    "                (Right,Bottom)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e206091",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-20T17:41:57.421498Z",
     "iopub.status.idle": "2025-09-20T17:41:57.421775Z",
     "shell.execute_reply": "2025-09-20T17:41:57.421633Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.421621Z"
    },
    "id": "7e206091",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = [\n",
    "    # Imagem 1\n",
    "    {\n",
    "        # [cx, cy, w, h]\n",
    "        \"boxes\": torch.tensor([[250, 30, 400, 150], [50, 40, 300, 190]], dtype=torch.float32),\n",
    "        \"scores\": torch.tensor([0.9, 0.75], dtype=torch.float32),\n",
    "        \"labels\": torch.tensor([0, 1], dtype=torch.int32),\n",
    "    },\n",
    "    # Imagem 2\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[100, 200, 250, 350]], dtype=torch.float32),\n",
    "        \"scores\": torch.tensor([0.8], dtype=torch.float32),\n",
    "        \"labels\": torch.tensor([1], dtype=torch.int32),\n",
    "    },\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[100, 300, 250, 350]], dtype=torch.float32),\n",
    "        \"scores\": torch.tensor([0.8], dtype=torch.float32),\n",
    "        \"labels\": torch.tensor([1], dtype=torch.int32),\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "targets = [\n",
    "    # Imagem 1 - Ground Truth\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[260, 40, 390, 160]], dtype=torch.float32),\n",
    "        \"labels\": torch.tensor([0], dtype=torch.int32),\n",
    "    },\n",
    "    # Imagem 2 - Ground Truth\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[110, 210, 240, 360], [10, 10, 50, 50]], dtype=torch.float32),\n",
    "        \"labels\": torch.tensor([1, 0], dtype=torch.int32),\n",
    "    },\n",
    "    {\n",
    "        \"boxes\": torch.tensor([[110, 300, 240, 360], [10, 10, 50, 50]], dtype=torch.float32),\n",
    "        \"labels\": torch.tensor([1, 0], dtype=torch.int32),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca273ef0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-20T17:41:57.422834Z",
     "iopub.status.idle": "2025-09-20T17:41:57.423126Z",
     "shell.execute_reply": "2025-09-20T17:41:57.422985Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.422972Z"
    },
    "id": "ca273ef0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def generate_realistic_detection_data(\n",
    "    num_images: int = 3,\n",
    "    image_size: tuple[int, int] = (800, 600),\n",
    "    num_classes: int = 2,\n",
    "    max_gt_boxes: int = 4,\n",
    "    # --- Novos parâmetros para controlar o realismo ---\n",
    "    detection_chance: float = 0.9,      # 90% de chance de detectar um objeto que existe\n",
    "    correct_class_chance: float = 0.95, # 95% de chance de classificar corretamente\n",
    "    max_noise_factor: float = 0.25,     # Ruído máximo de 15% nas coordenadas da caixa\n",
    "    max_false_positives: int = 20,       # Máximo de 2 detecções \"fantasmas\" por imagem\n",
    "):\n",
    "    \"\"\"\n",
    "    Gera dados realistas de predição e ground truth, onde as predições\n",
    "    são baseadas nos alvos (targets) com ruído e imprecisões.\n",
    "\n",
    "    Args:\n",
    "        num_images (int): O número de imagens para gerar dados.\n",
    "        image_size (tuple[int, int]): As dimensões da imagem (largura, altura).\n",
    "        num_classes (int): O número total de classes de objetos.\n",
    "        max_gt_boxes (int): O número máximo de caixas de ground truth por imagem.\n",
    "        detection_chance (float): Probabilidade de um objeto do ground truth ser detectado.\n",
    "        correct_class_chance (float): Probabilidade da classe prevista ser a correta.\n",
    "        max_noise_factor (float): Fator máximo de ruído para as coordenadas da caixa.\n",
    "        max_false_positives (int): Número máximo de detecções falsas (sem alvo correspondente).\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[dict], list[dict]]: Uma tupla contendo a lista de predições (preds)\n",
    "                                       e a lista de alvos (targets).\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    img_width, img_height = image_size\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        # 1. Gera os dados de Ground Truth (targets) como antes\n",
    "        num_target_boxes = random.randint(1, max_gt_boxes)\n",
    "        target_boxes = []\n",
    "        target_labels = []\n",
    "        for _ in range(num_target_boxes):\n",
    "            w = random.randint(50, img_width // 3)\n",
    "            h = random.randint(50, img_height // 3)\n",
    "            cx = random.randint(w // 2, img_width - w // 2)\n",
    "            cy = random.randint(h // 2, img_height - h // 2)\n",
    "            target_boxes.append([cx, cy, w, h])\n",
    "            target_labels.append(random.randint(0, num_classes - 1))\n",
    "\n",
    "        targets_dict = {\n",
    "            \"boxes\": torch.tensor(target_boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(target_labels, dtype=torch.int32),\n",
    "        }\n",
    "        all_targets.append(targets_dict)\n",
    "\n",
    "        # 2. Gera as Predições (preds) baseadas nos Targets\n",
    "        pred_boxes = []\n",
    "        pred_scores = []\n",
    "        pred_labels = []\n",
    "\n",
    "        # Itera sobre cada objeto real (ground truth)\n",
    "        for gt_box, gt_label in zip(target_boxes, target_labels):\n",
    "            # Simula falhas de detecção (Falsos Negativos)\n",
    "            if random.random() > detection_chance:\n",
    "                continue # O modelo \"não viu\" este objeto\n",
    "\n",
    "            # O objeto foi detectado, agora adicionamos imprecisões\n",
    "            base_score = 1.0\n",
    "\n",
    "            # Adiciona ruído à caixa (imprecisão de localização)\n",
    "            gt_cx, gt_cy, gt_w, gt_h = gt_box\n",
    "            noise_w = gt_w * max_noise_factor\n",
    "            noise_h = gt_h * max_noise_factor\n",
    "\n",
    "            pred_cx = gt_cx + random.uniform(-noise_w, noise_w)\n",
    "            pred_cy = gt_cy + random.uniform(-noise_h, noise_h)\n",
    "            pred_w = gt_w + random.uniform(-noise_w, noise_w)\n",
    "            pred_h = gt_h + random.uniform(-noise_h, noise_h)\n",
    "\n",
    "            pred_boxes.append([pred_cx, pred_cy, pred_w, pred_h])\n",
    "\n",
    "            # O score diminui com base na imprecisão\n",
    "            base_score -= random.uniform(0.0, 0.25)\n",
    "\n",
    "            # Simula erros de classificação\n",
    "            if random.random() > correct_class_chance:\n",
    "                # Gera uma classe diferente da correta\n",
    "                possible_labels = list(range(num_classes))\n",
    "                possible_labels.remove(gt_label)\n",
    "                pred_labels.append(random.choice(possible_labels))\n",
    "                base_score -= random.uniform(0.1, 0.3) # Penalidade maior por errar a classe\n",
    "            else:\n",
    "                pred_labels.append(gt_label)\n",
    "\n",
    "            pred_scores.append(round(max(0.3, base_score), 2))\n",
    "\n",
    "        # 3. Simula detecções extras (Falsos Positivos)\n",
    "        num_false_positives = random.randint(0, max_false_positives)\n",
    "        for _ in range(num_false_positives):\n",
    "            w = random.randint(40, img_width // 4)\n",
    "            h = random.randint(40, img_height // 4)\n",
    "            cx = random.randint(w // 2, img_width - w // 2)\n",
    "            cy = random.randint(h // 2, img_height - h // 2)\n",
    "            pred_boxes.append([cx, cy, w, h])\n",
    "            pred_labels.append(random.randint(0, num_classes - 1))\n",
    "            pred_scores.append(round(random.uniform(0.3, 0.65), 2)) # Scores mais baixos para FPs\n",
    "\n",
    "        preds_dict = {\n",
    "            \"boxes\": torch.tensor(pred_boxes, dtype=torch.float32),\n",
    "            \"scores\": torch.tensor(pred_scores, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(pred_labels, dtype=torch.int32),\n",
    "        }\n",
    "        all_preds.append(preds_dict)\n",
    "\n",
    "    return all_preds, all_targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c329313",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-20T17:41:57.424324Z",
     "iopub.status.idle": "2025-09-20T17:41:57.424612Z",
     "shell.execute_reply": "2025-09-20T17:41:57.424460Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.424449Z"
    },
    "id": "7c329313",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds, targets = generate_realistic_detection_data(\n",
    "    num_images=6000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af7512",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-20T17:41:57.425803Z",
     "iopub.status.idle": "2025-09-20T17:41:57.426091Z",
     "shell.execute_reply": "2025-09-20T17:41:57.425933Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.425920Z"
    },
    "id": "50af7512",
    "outputId": "7b954413-52cc-4d9b-dc91-87c1e4f56df7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "metric = MeanAveragePrecision(box_format=\"cxcywh\")\n",
    "metric.update(preds=preds, target=targets)\n",
    "\n",
    "print(metric.compute()[\"map_50\"].item())\n",
    "print(f\"{mean_average_precision(preds, targets, 20) :.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c713edb",
   "metadata": {
    "id": "6c713edb"
   },
   "source": [
    "## Teste IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dceb864",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-20T17:41:57.427089Z",
     "iopub.status.idle": "2025-09-20T17:41:57.427370Z",
     "shell.execute_reply": "2025-09-20T17:41:57.427237Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.427228Z"
    },
    "id": "8dceb864",
    "outputId": "e27b3845-2f06-4251-a1bf-91d8315e1c6f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "predicted_box = torch.tensor(\n",
    "    [1, 1, 2, 2],\n",
    ")\n",
    "\n",
    "# Caixas de verdade-fundamental (duas caixas para comparar)\n",
    "ground_truth_boxes = torch.tensor([\n",
    "    # Caixa 1: Grande sobreposição com a predição\n",
    "    [2, 2, 2, 2],\n",
    "    # Caixa 2: Nenhuma sobreposição com a predição\n",
    "    [-1, -1, 1, 1],\n",
    "    [-1, -1, 1, 1],\n",
    "    [-1, -1, 1, 1],\n",
    "    [2, 2, 2, 2],\n",
    "])\n",
    "\n",
    "iou(predicted_box, ground_truth_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518587c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-20T17:41:57.428820Z",
     "iopub.status.idle": "2025-09-20T17:41:57.429112Z",
     "shell.execute_reply": "2025-09-20T17:41:57.428969Z",
     "shell.execute_reply.started": "2025-09-20T17:41:57.428956Z"
    },
    "id": "b518587c",
    "outputId": "85f77995-084e-4d20-b4a3-06af982c3d0f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = torch.rand([1, 3, IMG_DIM, IMG_DIM])\n",
    "yolo(x)[0][..., 5:].argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1565b",
   "metadata": {},
   "source": [
    "# Dataset UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1f2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_images = [\"2009_005069_2_8\", \"2009_004969_1_0\", \"2011_002863_0_4\", \"2009_000455_2_14\", \"2009_004969_2_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85367c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_(path):\n",
    "    for image_name in banned_images:\n",
    "        if image_name in path:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Dataset para carregar as imagens---------------------\n",
    "class UNetDataset(Dataset):\n",
    "\n",
    "    #inicializa o dataset\n",
    "    def __init__(self, type_, dim = 416, unet_dataset_path = [\"unet_dataset\"], device = \"cpu\"):\n",
    "        #Carrega o caminho do conjunto desejado (val, train, test)\n",
    "        self.base_path_data = os.path.join(*unet_dataset_path, \"images\", f\"{type_}\")\n",
    "        self.base_path_labels = os.path.join(*unet_dataset_path, \"labels\", f\"{type_}\")\n",
    "        \n",
    "        #self.images = glob(os.path.join(self.base_path_data, \"*.jpg\"))\n",
    "        self.images = list(filter(filter_, glob(os.path.join(self.base_path_data, \"*.jpg\"))))\n",
    "        self.device = device\n",
    "        self.dim = dim\n",
    "\n",
    "        self.classes = [\"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "    \n",
    "        self.toTensor = ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def letterbox_data(self, image, label):\n",
    "        if image.shape[2] > image.shape[1]: #W > H\n",
    "            scale_factor = self.dim/image.shape[2]\n",
    "            new_H = int(scale_factor * image.shape[1])\n",
    "            new_dims = (new_H, self.dim)\n",
    "\n",
    "            resize_i = T.Resize(new_dims)\n",
    "            resize_l = T.Resize(new_dims, interpolation = T.InterpolationMode.NEAREST)\n",
    "            \n",
    "            a = int(round((self.dim - new_H)/2 - 0.1))\n",
    "            b = int(round((self.dim - new_H)/2 + 0.1))\n",
    "            padding = T.Pad([0, a, 0, b], fill = 117)\n",
    "\n",
    "        elif image.shape[2] < image.shape[1]: #W < H\n",
    "            scale_factor = self.dim/image.shape[1]\n",
    "            new_W = int(scale_factor * image.shape[2])\n",
    "            new_dims = (self.dim, new_W)\n",
    "\n",
    "            resize_i = T.Resize(new_dims)\n",
    "            resize_l = T.Resize(new_dims, interpolation = T.InterpolationMode.NEAREST)\n",
    "\n",
    "            a = int(round((self.dim - new_W)/2 - 0.1))\n",
    "            b = int(round((self.dim - new_W)/2 + 0.1))\n",
    "            padding = T.Pad([a, 0, b, 0], fill = 117)\n",
    "\n",
    "        else: #W = H\n",
    "            resize_i = T.Resize((self.dim, self.dim))\n",
    "            resize_l = T.Resize((self.dim, self.dim), interpolation = T.InterpolationMode.NEAREST)\n",
    "\n",
    "            padding = lambda x: x\n",
    "\n",
    "        return padding(resize_i(image)), padding(resize_l(label))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        class_ = int(name.split(\"_\")[-1])\n",
    "\n",
    "        label_path = os.path.join(self.base_path_labels, f\"{name}.png\")\n",
    "        label = Image.open(label_path).convert(\"RGB\")\n",
    "\n",
    "        colors = label.getcolors(label.size[0] * label.size[1])\n",
    "        most_frequent_colors = sorted(colors, key = lambda item: item[0], reverse = True)\n",
    "        try:\n",
    "            if most_frequent_colors[0][1] != (0, 0, 0) and most_frequent_colors[0][1] != (224, 224, 192):\n",
    "                r, g, b = most_frequent_colors[0][1]\n",
    "            elif most_frequent_colors[1][1] != (0, 0, 0) and most_frequent_colors[1][1] != (224, 224, 192):\n",
    "                r, g, b = most_frequent_colors[1][1]\n",
    "            else:\n",
    "                r, g, b = most_frequent_colors[2][1]\n",
    "        except:\n",
    "            print(name)\n",
    "            return 0\n",
    "        #Faz uma array de lookup pra converter o rgb pra classe mais eficientemente\n",
    "        lookup_classes = np.ones((256, 256, 256), dtype = \"uint8\") * len(self.classes)\n",
    "        lookup_classes[r, g, b] = class_\n",
    "\n",
    "        image = decode_image(img_path).to(torch.float).to(self.device)\n",
    "        label = self.toTensor(label) * 255\n",
    "\n",
    "        image, label = self.letterbox_data(image, label.to(torch.uint8))\n",
    "\n",
    "        label_permute = label.permute(1, 2, 0)\n",
    "        r_channel = label_permute[:, :, 0]\n",
    "        g_channel = label_permute[:, :, 1]\n",
    "        b_channel = label_permute[:, :, 2]\n",
    "\n",
    "        label = torch.Tensor(lookup_classes[r_channel, g_channel, b_channel]).to(self.device)\n",
    "        return image, label.to(int)\n",
    "\n",
    "    #Função auxiliar para pegar a imagem original sem normalização\n",
    "    def get_plot_image(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        return decode_image(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_u = UNetDataset(\"train\", 256, unet_dataset_path = [UNET_DATASET_PATH], device = device)\n",
    "val_data_u = UNetDataset(\"val\", 256, unet_dataset_path = [UNET_DATASET_PATH], device = device)\n",
    "train_data_u[0][0].shape, torch.unique(train_data_u[0][1], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e27c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_u = DataLoader(train_data_u, batch_size = BATCH_SIZE, shuffle = True)\n",
    "val_dataloader_u = DataLoader(val_data_u, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(model, dataset, index, deeplab = False):\n",
    "    with torch.no_grad():\n",
    "        if not deeplab:\n",
    "            pred = model(dataset[index][0]).cpu()\n",
    "        else:\n",
    "            pred = model(dataset[index][0].unsqueeze(0))[0].cpu()\n",
    "        pred_softmax = pred.argmax(axis = 0)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize = (10, 24))\n",
    "\n",
    "    axs[0].imshow(dataset.get_plot_image(index).permute(1, 2, 0).to(int).cpu())\n",
    "    axs[0].set_title(\"Original image\")\n",
    "\n",
    "    axs[1].imshow(colors[pred_softmax].cpu())\n",
    "    axs[1].set_title(\"Prediction\")\n",
    "\n",
    "    axs[2].imshow(colors[dataset[index][1].to(int).cpu()].to(int).cpu())\n",
    "    axs[2].set_title(\"Original label\")\n",
    "    #train_data[0][1].unique(return_counts = True)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c30ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------UNet incial----------------------------------\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, index = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.index = index\n",
    "        out_channels = 2 ** (5 + index)\n",
    "        in_channels = out_channels//2 if index != 1 else 3\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
    "        self.conv_2 = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        y = self.max_pool(x)\n",
    "        return x, y\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, index = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.index = index\n",
    "        in_channels = 2 ** (6 + index)\n",
    "        out_channels = in_channels//2\n",
    "\n",
    "        self.transpose = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
    "        self.conv_1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
    "        self.conv_2 = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.transpose(x)\n",
    "        cat_dim = len(x.shape) - 3\n",
    "        x = torch.cat((skip, x), dim = cat_dim)\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module, Trainer):\n",
    "    def __init__(self, criterion, plot_func, depth = 4, num_classes = 21):\n",
    "        super().__init__()\n",
    "        Trainer.__init__(self, criterion, plot_func)\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.convs.append(ConvBlock(i + 1).to(device))\n",
    "\n",
    "        in_channels = 2 ** (5 + depth)\n",
    "        out_channels = in_channels * 2\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\"), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.ups.insert(0, UpBlock(i + 1).to(device))\n",
    "\n",
    "        self.classifier = nn.Conv2d(in_channels = 64, out_channels = num_classes, kernel_size = 1, padding = \"same\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for block in self.convs:\n",
    "            conv_out, x = block(x)\n",
    "            skip_connections.insert(0, conv_out)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i, block in enumerate(self.ups):\n",
    "            x = block(x, skip_connections[i])\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------Focal Loss-------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma = 2, weight = None) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "    def __call__(self, inputs, targets):\n",
    "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none', weight = self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Inferência básica----------------------------------\n",
    "unet = UNet(FocalLoss(), display_image).to(device)\n",
    "pred = unet(val_data_u[0][0])\n",
    "pred_softmax = pred.argmax(axis = 0)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = torch.tensor([[64 , 128, 64],\n",
    "[192, 0  , 128],\n",
    "[0  , 128, 192],\n",
    "[0  , 128, 64],\n",
    "[128, 0  , 0],\n",
    "[64 , 0  , 128],\n",
    "[64 , 0  , 192],\n",
    "[192, 128, 64],\n",
    "[192, 192, 128],\n",
    "[64 , 64 , 128],\n",
    "[128, 0  , 192],\n",
    "[192, 0  , 64],\n",
    "[128, 128, 64],\n",
    "[192, 0  , 192],\n",
    "[128, 64 , 64],\n",
    "[64 , 192, 128],\n",
    "[64 , 64 , 0],\n",
    "[128, 64 , 128],\n",
    "[128, 128, 192],\n",
    "[0  , 0  , 192],\n",
    "[0, 0, 0]], dtype = torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c323ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------Plot de imagens----------------------------\n",
    "display_image(unet, train_data_u, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(unet.parameters(), lr = 0.001)\n",
    "unet.fit(train_dataloader_u, val_dataloader_u, optimizer, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00ec7d",
   "metadata": {},
   "source": [
    "# Dataset MaskR-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dbdd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.images_dir = os.path.join(root_dir, 'images', 'train')\n",
    "        self.labels_dir = os.path.join(root_dir, 'labels', 'train')\n",
    "        self.masks_dir = os.path.join(root_dir, 'masks', 'train')\n",
    "        \n",
    "        self.image_paths = sorted(glob.glob(os.path.join(self.images_dir, \"*\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        img_id = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        masks = []\n",
    "\n",
    "        label_files = sorted(glob.glob(os.path.join(self.labels_dir, f\"{img_id}_*.txt\")))\n",
    "        mask_files = sorted(glob.glob(os.path.join(self.masks_dir, f\"{img_id}_*.png\")))\n",
    "\n",
    "        for label_file, mask_file in zip(label_files, mask_files):\n",
    "            with open(label_file, 'r') as f:\n",
    "                data = f.read().strip().split()\n",
    "                class_id = int(data[0])\n",
    "                box_coords = [float(coord) for coord in data[1:]]\n",
    "                \n",
    "                labels.append(class_id)\n",
    "                boxes.append(box_coords)\n",
    "\n",
    "            mask = Image.open(mask_file)\n",
    "            mask_np = np.array(mask)\n",
    "            \n",
    "            binary_mask = (mask_np > 0).astype(np.uint8)\n",
    "            masks.append(binary_mask)\n",
    "        \n",
    "        if len(boxes) > 0:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "        else:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels = torch.zeros(0, dtype=torch.int64)\n",
    "\n",
    "            h, w = img.size[1], img.size[0]\n",
    "            masks = torch.zeros((0, h, w), dtype=torch.uint8)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "            \n",
    "        return img, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "data_transforms = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "train_dataset = MaskDataset(root_dir=mask_dataset_path, transforms=data_transforms)\n",
    "    \n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51801bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_finetune_model(num_classes):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "maskrcnn = get_finetune_model(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542937bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "maskrcnn.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "params = [p for p in maskrcnn.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    maskrcnn.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, (images, targets) in enumerate(train_data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_dict = maskrcnn(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += losses.item()\n",
    "        \n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"  Batch [{i+1}/{len(train_data_loader)}], Loss: {losses.item():.4f}\")\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_data_loader)\n",
    "    print(epoch_loss)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1271215,
     "isSourceIdPinned": false,
     "sourceId": 2118595,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "studies",
   "language": "python",
   "name": "studies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
