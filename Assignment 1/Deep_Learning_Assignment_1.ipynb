{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gOU8w75UkCl",
        "outputId": "efdb8374-0f64-406a-edd7-c1a610719781"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "camvid_path = kagglehub.dataset_download('carlolepelaars/camvid')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dfvgCx6UkCn"
      },
      "source": [
        "\n",
        "# **Programming Assignment 1 - Semantic Segmentation**\n",
        "\n",
        "**Professor**: Dário Oliveira  \n",
        "**Monitor**: Lívia Meinhardt\n",
        "\n",
        "Neste trabalho prático, vocês irão investigar o desempenho de modelos de segmentação semântica, utilizando redes U-Net e DeepLabV3. A proposta vai além de treinar modelos: vocês deverão analisar onde eles funcionam bem (ou não), interpretar os erros e justificar suas decisões com base nos conceitos vistos em aula.\n",
        "\n",
        "\n",
        "# **Instruções:**\n",
        "\n",
        "1. **Escolha do Ambiente de Execução**:  \n",
        "   Utilize Google Colab ou Kaggle Notebook. Recomendamos iniciar seu notebook no Kaggle diretamente da página do dataset [CamVid](https://www.kaggle.com/datasets/carlolepelaars/camvid).\n",
        "\n",
        "3. **Criação de um Dataset Customizado**:  \n",
        "   As máscaras vêm em formato RGB. Use o CSV fornecido para converter as cores em labels (classe por pixel). Crie seu próprio Dataset em PyTorch.\n",
        "\n",
        "4. **Construção da Arquitetura U-Net**:  \n",
        "   Implemente uma U-Net (ou ResUNet), com a possibilidade de variar sua profundidade (número de blocos codificadores/decodificadores). Explore como isso impacta o desempenho e a quantidade de parâmetros.\n",
        "\n",
        "   <div>\n",
        "   <img src=\"https://camo.githubusercontent.com/6b548ee09b97874014d72903c891360beb0989e74b4585249436421558faa89d/68747470733a2f2f692e696d6775722e636f6d2f6a6544567071462e706e67\" width=600>\n",
        "   </div>\n",
        "\n",
        "5. **Função de Treinamento**:  \n",
        "   Crie uma função de treinamento e registre métricas (loss, acurácia) em cada época. A cada 5 ou 10 épocas, visualize uma predição (imagem original, máscara verdadeira e predita).\n",
        "\n",
        "6. **Experimentação**:\n",
        "    Além da profundidade da rede e como ela afeta desempenho e quantidade de parâmetros, explore pelo menos dois otimizadores e funções de perda adequadas para segmentação semântica. Fundamente as escolhas e explique os resultados de acordo com a teoria vista em aula.\n",
        "\n",
        "8. **Avaliação**:\n",
        "   Implemente as métricas por classe: Precisão e IoU. Além da média geral, use para identificar as classes com pior desempenho e caracterizar seu melhor modelo.\n",
        "\n",
        "9. **Explicabilidade com Mapas de Erro**:\n",
        "   Escolha imagens com desempenho ruim (menor IoU ou precisão) e gere mapas de erro. Analise visualmente onde o modelo erra (bordas, classes confundidas, objetos pequenos, etc.). Relacione os erros às métricas.\n",
        "\n",
        "10. **Data Augmentation**:  \n",
        "     Implemente alguma forma de data augmentation. Avalie se houve ganho em desempenho. Justifique.\n",
        "\n",
        "11. **Fine-Tuning do DeepLabV3**:\n",
        "   Realize o fine-tuning do modelo [DeepLabV3](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html#torchvision.models.segmentation.deeplabv3_resnet50) pré-treinado ajustando `classifier` e `aux_classifier` para o número de classes do seu dataset. Congele o backbone conforme necessário e treine o modelo. Compare seu desempenho com a melhor U-Net em termos de métricas, número de parâmetros e qualidade visual das predições.\n",
        "\n",
        "12. **Apresentação Final**:  \n",
        "    Ao final, prepare uma apresentação resumindo os passos seguidos, resultados obtidos, gráficos de perdas e acurácia, e discussões sobre o desempenho do modelo. Lembre de fundamentar a discussão com os aspectos teoricos vistos em sala de aula.\n",
        "\n",
        "\n",
        "### **Pontos Importantes:**\n",
        "\n",
        "- Escolher adequadamente o tamanho do BATCH, Loss Function e Otimizador e saber o motivo de cada escolha;\n",
        "- Monitore o uso das GPUs, o kaggle te informa quantidade de tempo disponível, mas o colab não;\n",
        "- Observe as classes com mais erros por parte do modelo;\n",
        "- Adicione gráficos de perda e acurácia na sua apresentação;\n",
        "- Coloque imagens das predições do modelo;\n",
        "- Use **Pytorch!!!**.\n",
        "\n",
        "Note que as instruções acima são os requisitos da entrega, mas não precisam ser feitas exatamente nesta ordem. Você pode implementar o lógica de validação do modelo, testar uma versão inicial e modificar conforme os resultados obtidos. Lembre-se de fundamentas suas escolhas e fluxo de trabalho na apresentação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "execution": {
          "iopub.execute_input": "2025-09-07T17:47:54.065711Z",
          "iopub.status.busy": "2025-09-07T17:47:54.065426Z",
          "iopub.status.idle": "2025-09-07T17:48:06.838026Z",
          "shell.execute_reply": "2025-09-07T17:48:06.836932Z",
          "shell.execute_reply.started": "2025-09-07T17:47:54.065689Z"
        },
        "id": "GGfWsb5pUkCq",
        "outputId": "df016ddc-ecf3-4c6b-a880-c9f32c8d6f57",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision.io import decode_image\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead, FCNHead\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"using device '{device}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T17:48:06.839725Z",
          "iopub.status.busy": "2025-09-07T17:48:06.8393Z",
          "iopub.status.idle": "2025-09-07T17:48:08.909174Z",
          "shell.execute_reply": "2025-09-07T17:48:08.90834Z",
          "shell.execute_reply.started": "2025-09-07T17:48:06.839702Z"
        },
        "id": "F4Kc4KJQUkCr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#----------------------------I/O no Kaggle------------------------------\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        continue\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-09-07T17:52:32.648449Z",
          "iopub.status.busy": "2025-09-07T17:52:32.64805Z",
          "iopub.status.idle": "2025-09-07T17:52:32.724985Z",
          "shell.execute_reply": "2025-09-07T17:52:32.724108Z",
          "shell.execute_reply.started": "2025-09-07T17:52:32.648422Z"
        },
        "id": "BAmPKkDGUkCs",
        "outputId": "13fae08f-003d-4a40-9c6f-bc8f7ff99b29",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#------------------Dataset para carregar as imagens---------------------\n",
        "class CamVidDataset(Dataset):\n",
        "\n",
        "    #inicializa o dataset\n",
        "    def __init__(self, type_, img_dim = [720, 960], dataset_path = (\"/\", \"kaggle\", \"input\", \"camvid\"), device = \"cpu\"):\n",
        "        #Carrega o caminho do conjunto desejado (val, train, test)\n",
        "        self.base_path_data = os.path.join(*dataset_path, \"CamVid\", f\"{type_}\")\n",
        "        self.base_path_labels = os.path.join(*dataset_path, \"CamVid\", f\"{type_}_labels\")\n",
        "        self.data = glob.glob(os.path.join(self.base_path_data, \"*.png\"))\n",
        "\n",
        "        #Carrega o csv com as classes e trata ele\n",
        "        df = pd.read_csv(os.path.join(*dataset_path, \"CamVid\", \"class_dict.csv\"))\n",
        "        df = df.reset_index().rename(columns = {\"index\": \"class\"})\n",
        "        self.df = df.set_index([\"r\", \"g\", \"b\"])\n",
        "\n",
        "        #Faz uma array de lookup pra converter o rgb pra classe mais eficientemente\n",
        "        self.lookup_classes = np.zeros((256, 256, 256), dtype = \"uint8\")\n",
        "        for (r, g, b), row in self.df.iterrows():\n",
        "            self.lookup_classes[r, g, b] = row[\"class\"]\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.resize_img = T.Resize(size=img_dim, interpolation=T.InterpolationMode.BILINEAR, antialias=True)\n",
        "        self.resize_label = T.Resize(size=img_dim, interpolation=T.InterpolationMode.NEAREST)\n",
        "        self.normalize_img = T.Normalize([105.3549, 107.8312, 109.6686], [5637.0488**0.5, 5869.9844**0.5, 5731.8906**0.5])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Pega o path da imagem e da label e carrega\n",
        "        img_path = self.data[idx]\n",
        "        name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "        label_path = os.path.join(self.base_path_labels, f\"{name}_L.png\")\n",
        "\n",
        "        image = decode_image(img_path).to(torch.float).to(self.device)\n",
        "        label = decode_image(label_path)\n",
        "\n",
        "        image = self.resize_img(image)\n",
        "        image = self.normalize_img(image)\n",
        "        label = self.resize_label(label)\n",
        "\n",
        "        #Procura na array de lookup as classes de cada pixel em um acesso só\n",
        "        #(mais eficiente)\n",
        "        label_permute = label.permute(1, 2, 0)\n",
        "        r_channel = label_permute[:, :, 0]\n",
        "        g_channel = label_permute[:, :, 1]\n",
        "        b_channel = label_permute[:, :, 2]\n",
        "        label = torch.Tensor(self.lookup_classes[r_channel, g_channel, b_channel]).to(self.device)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def orig_image(self, idx):\n",
        "        img_path = self.data[idx]\n",
        "        return decode_image(img_path)\n",
        "\n",
        "train_data = CamVidDataset(\"train\", img_dim=[256, 256], dataset_path = [camvid_path], device = device)\n",
        "val_data = CamVidDataset(\"val\", img_dim=[256, 256], dataset_path = [camvid_path], device = device)\n",
        "test_data = CamVidDataset(\"test\", img_dim=[256, 256], dataset_path = [camvid_path], device = device)\n",
        "test_data[0][0].dtype, test_data[0][1].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lf4Rm3kmdvrs",
        "outputId": "223f856e-c018-439b-ef77-7ca31b07c2f8"
      },
      "outputs": [],
      "source": [
        "#---------------Calculo a frequência das classes--------------------------\n",
        "classes = pd.read_csv(os.path.join(camvid_path, \"CamVid\", \"class_dict.csv\"))\n",
        "colors = torch.Tensor(classes[[\"r\", \"g\", \"b\"]].to_numpy()).to(int).to(device)\n",
        "\n",
        "freqs = torch.zeros((32,)).to(int).to(device)\n",
        "for image, label in train_data:\n",
        "    class_, count = label.unique(return_counts = True)\n",
        "    freqs[class_.to(int)] += count\n",
        "\n",
        "classes[\"frequencies\"] = freqs.cpu()\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl6vxavBUkCt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#---------------------------UNet incial----------------------------------\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, index = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.index = index\n",
        "        out_channels = 2 ** (5 + index)\n",
        "        in_channels = out_channels//2 if index != 1 else 3\n",
        "\n",
        "        self.conv_1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
        "        self.conv_2 = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv_1(x))\n",
        "        x = self.relu(self.conv_2(x))\n",
        "        y = self.max_pool(x)\n",
        "        return x, y\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, index = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.index = index\n",
        "        in_channels = 2 ** (6 + index)\n",
        "        out_channels = in_channels//2\n",
        "\n",
        "        self.transpose = nn.ConvTranspose2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 2, stride = 2)\n",
        "        self.conv_1 = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
        "        self.conv_2 = nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\")\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x = self.transpose(x)\n",
        "        cat_dim = len(x.shape) - 3\n",
        "        x = torch.cat((skip, x), dim = cat_dim)\n",
        "        x = self.relu(self.conv_1(x))\n",
        "        x = self.relu(self.conv_2(x))\n",
        "        return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, depth = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.convs.append(ConvBlock(i + 1).to(device))\n",
        "\n",
        "        in_channels = 2 ** (5 + depth)\n",
        "        out_channels = in_channels * 2\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = out_channels, out_channels = out_channels, kernel_size = 3, padding = \"same\"), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            self.ups.insert(0, UpBlock(i + 1).to(device))\n",
        "\n",
        "        self.classifier = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 1, padding = \"same\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        for block in self.convs:\n",
        "            conv_out, x = block(x)\n",
        "            skip_connections.insert(0, conv_out)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        for i, block in enumerate(self.ups):\n",
        "            x = block(x, skip_connections[i])\n",
        "\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czzAxhDwdvrt",
        "outputId": "53d6587b-9afa-4de7-dc88-672b9481d797"
      },
      "outputs": [],
      "source": [
        "#-------------------------Inferência básica----------------------------------\n",
        "base_model = UNet().to(device)\n",
        "pred = base_model(test_data[0][0])\n",
        "pred_softmax = pred.argmax(axis = 0)\n",
        "print(pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---------------------Plot de imagens----------------------------\n",
        "def display_image(model, dataset, index, deeplab = False):\n",
        "    with torch.no_grad():\n",
        "        if not deeplab:\n",
        "            pred = model(dataset[index][0])\n",
        "        else:\n",
        "            pred = model(dataset[index][0].unsqueeze(0))[0]\n",
        "        pred_softmax = pred.argmax(axis = 0)\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize = (10, 24))\n",
        "\n",
        "    axs[0].imshow(dataset.orig_image(index).permute(1, 2, 0).to(int).cpu())\n",
        "    axs[0].set_title(\"Original image\")\n",
        "\n",
        "    axs[1].imshow(colors[pred_softmax].cpu())\n",
        "    axs[1].set_title(\"Prediction\")\n",
        "\n",
        "    axs[2].imshow(colors[dataset[index][1].to(int)].to(int).cpu())\n",
        "    axs[2].set_title(\"Original label\")\n",
        "    #train_data[0][1].unique(return_counts = True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "display_image(model_4, test_data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SydeFocKdvru"
      },
      "outputs": [],
      "source": [
        "#--------------------Loops de treino e teste-------------\n",
        "def loop_treino(train_dataloader, val_dataloader, modelo, loss_fc, otimizador, scheduler, epochs = 30):\n",
        "    modelo.train()\n",
        "    losses = {\n",
        "        \"train\": {\"acc\": [], \"loss\": []},\n",
        "        \"val\": {\"acc\": [], \"loss\": []}\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        #------Treino----\n",
        "        running_loss_train, correct_predictions_train, total_samples = 0.0, 0, 0\n",
        "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1} | train\")\n",
        "        for X, y in progress_bar:\n",
        "            pred = modelo(X)\n",
        "            loss = loss_fc(pred, y.to(int))\n",
        "\n",
        "            otimizador.zero_grad()\n",
        "            loss.backward()\n",
        "            otimizador.step()\n",
        "\n",
        "            running_loss_train += loss.item() * X.size(0)\n",
        "            _, predicted_labels = torch.max(pred, 1)\n",
        "            correct_predictions_train += (predicted_labels == y).sum().item()\n",
        "            total_samples += torch.numel(y)\n",
        "\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_loss_train = running_loss_train / len(train_dataloader.dataset)\n",
        "        epoch_acc_train = correct_predictions_train / total_samples\n",
        "        #-----Validação----\n",
        "        running_loss_val, correct_predictions_val, total_samples = 0.0, 0, 0\n",
        "        progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1} | val\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in progress_bar:\n",
        "                pred = modelo(X)\n",
        "                loss = loss_fc(pred, y.to(int))\n",
        "\n",
        "                running_loss_val += loss.item() * X.size(0)\n",
        "                _, predicted_labels = torch.max(pred, 1)\n",
        "                correct_predictions_val += (predicted_labels == y).sum().item()\n",
        "                total_samples += torch.numel(y)\n",
        "\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        epoch_loss_val = running_loss_val / len(val_dataloader.dataset)\n",
        "        epoch_acc_val = correct_predictions_val / total_samples\n",
        "        #-----Display----\n",
        "        scheduler.step()\n",
        "        print(f\"Fim Epoch {epoch+1}:\")\n",
        "        print(f\"   -> Loss train: {epoch_loss_train:.6f} | Loss val: {epoch_loss_val:.6f}\")\n",
        "        print(f\"   -> Acc train: {epoch_acc_train:.6f}  | Acc val: {epoch_acc_val:.6f}\")\n",
        "        print(f\"   -> LR: {scheduler.get_last_lr()[0]:.6f}\\n\")\n",
        "\n",
        "        display_image(modelo, val_dataloader.dataset, 0)\n",
        "\n",
        "        losses[\"train\"][\"acc\"].append(epoch_acc_train)\n",
        "        losses[\"train\"][\"loss\"].append(epoch_loss_train)\n",
        "        losses[\"val\"][\"acc\"].append(epoch_acc_val)\n",
        "        losses[\"val\"][\"loss\"].append(epoch_loss_val)\n",
        "        \n",
        "    return losses\n",
        "\n",
        "\n",
        "def loop_teste(dataloader, modelo, loss_fc):\n",
        "    modelo.eval()\n",
        "\n",
        "    tamanho = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = modelo(X)\n",
        "            loss_test += loss_fc(pred, y).item()\n",
        "            acertos += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    loss_test /= num_batches\n",
        "    print(f\"Erro no Teste: \\n Perda média: {loss_test:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SEAdf0edvru"
      },
      "outputs": [],
      "source": [
        "#--------------Data loader, erro, otimizador, etc----------------------\n",
        "batch_size = 12\n",
        "train_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "val_dataloader = DataLoader(val_data, batch_size = batch_size, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#--------------Focal Loss-------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma = 2, weight = None) -> None:\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight\n",
        "\n",
        "    def __call__(self, inputs, targets):\n",
        "        ce_loss = torch.nn.functional.cross_entropy(inputs, targets, reduction='none', weight = self.weight)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#----------------UNet 4 layers CE Loss NAdam-------------------\n",
        "model_4_ce = UNet(4).to(device)\n",
        "optimizer = torch.optim.NAdam(model_4_ce.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=15, gamma=0.1)\n",
        "\n",
        "weights = classes[\"frequencies\"].sum()/(classes[\"frequencies\"] + 1e0)\n",
        "error_ce = nn.CrossEntropyLoss()#weight = torch.Tensor(weights).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "historico = loop_treino(train_dataloader, val_dataloader, model_4_ce, error_ce, optimizer, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model_4_cd.state_dict(), \"unet_4_ce_loss_nadam.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhD_LxflScl7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def model_eval(dataloader, model, num_classes, class_names):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    image_ious = []\n",
        "\n",
        "    full_cm = np.zeros((num_classes, num_classes))\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
        "    with torch.no_grad():\n",
        "        for i, (X, y) in enumerate(progress_bar):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            pred_logits = model(X)\n",
        "            pred_labels = torch.argmax(pred_logits, dim=1)\n",
        "            pred_labels = pred_labels.cpu().numpy().flatten()\n",
        "            \n",
        "            preds.append(pred_labels)\n",
        "            labels.append(y.cpu().numpy().flatten())\n",
        "            \n",
        "            cm = confusion_matrix(labels[-1], preds[-1], labels=range(num_classes))\n",
        "\n",
        "            TP = np.diag(cm)\n",
        "            FP = cm.sum(axis=0) - TP\n",
        "            FN = cm.sum(axis=1) - TP\n",
        "\n",
        "            iou_per_class = np.divide(TP, TP + FP + FN, out=np.zeros_like(TP, dtype=float), where=(TP + FP + FN) != 0)\n",
        "\n",
        "            union = TP + FP + FN\n",
        "\n",
        "            mean_iou_img = np.mean(iou_per_class[union > 0])\n",
        "            \n",
        "            image_ious.append((i, mean_iou_img))\n",
        "\n",
        "            full_cm += cm\n",
        "\n",
        "            progress_bar.set_postfix(mIoU=mean_iou_img)\n",
        "    \n",
        "    sorted_images = sorted(image_ious, key=lambda item: item[1])\n",
        "\n",
        "    TP = np.diag(full_cm)\n",
        "    FP = full_cm.sum(axis=0) - TP\n",
        "    FN = full_cm.sum(axis=1) - TP\n",
        "    TN = full_cm.sum() - (TP + FP + FN)\n",
        "\n",
        "    class_precision = np.divide(TP, TP + FP, out=np.zeros_like(TP, dtype=float), where=(TP + FP) != 0)\n",
        "    #class_acc = np.divide(TP + TN, TP + TN + FP + FN + 1, out=np.zeros_like(TP, dtype=float))\n",
        "    class_iou = np.divide(TP, TP + FP + FN, out=np.zeros_like(TP, dtype=float), where=(TP + FP + FN) != 0)\n",
        "\n",
        "    metrics_df = pd.DataFrame({\n",
        "        \"Classe\": class_names,\n",
        "        #'Acurácia': class_acc,\n",
        "        \"Precisão\": class_precision,\n",
        "        \"IoU\": class_iou\n",
        "    })\n",
        "\n",
        "    sorted_metrics_df = metrics_df.sort_values(by=\"IoU\", ascending=True)\n",
        "    print(sorted_metrics_df.to_string())\n",
        "    \n",
        "    return sorted_metrics_df, cm, sorted_images\n",
        "\n",
        "classes_list = classes[\"name\"].tolist()\n",
        "num_classes = len(classes_list)\n",
        "\n",
        "metrics_df, confusion_m, worst_imgs = model_eval(val_dataloader, model, num_classes, classes_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mapa de Erro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def plot_error_map(dataset, modelo, image_index, colors_tensor):\n",
        "    modelo.eval()\n",
        "    \n",
        "    img_tensor, label_tensor = dataset[image_index]\n",
        "    img_tensor = img_tensor.cpu()\n",
        "    label_tensor = label_tensor.cpu()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_logits = modelo(img_tensor.unsqueeze(0).to(device))\n",
        "        pred_tensor = torch.argmax(pred_logits, dim=1).squeeze(0).cpu()\n",
        "\n",
        "    gt_mask_np = colors_tensor[label_tensor.long()].numpy().astype(np.uint8)\n",
        "    pred_mask_np = colors_tensor[pred_tensor.long()].numpy().astype(np.uint8)\n",
        "\n",
        "    mean = torch.tensor([105.3549, 107.8312, 109.6686]).view(3, 1, 1)\n",
        "    std = torch.tensor([5637.0488**0.5, 5869.9844**0.5, 5731.8906**0.5]).view(3, 1, 1)\n",
        "    \n",
        "    original_img_tensor = img_tensor * std + mean\n",
        "    original_img_np = original_img_tensor.byte().permute(1, 2, 0).numpy()\n",
        "\n",
        "    error_mask = (pred_tensor != label_tensor).numpy()\n",
        "    error_overlay_np = original_img_np.copy()\n",
        "    error_overlay_np[error_mask] = [255, 0, 0]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 4)\n",
        "    \n",
        "    axes[0].imshow(original_img_np)\n",
        "    axes[0].set_title(f\"Original - idx: {image_index}\")\n",
        "\n",
        "    axes[1].imshow(gt_mask_np)\n",
        "    axes[1].set_title(\"True Mask (GT)\")\n",
        "\n",
        "    axes[2].imshow(pred_mask_np)\n",
        "    axes[2].set_title(\"Model Pred\")\n",
        "\n",
        "    axes[3].imshow(error_overlay_np)\n",
        "    axes[3].set_title(\"Error\")\n",
        "\n",
        "    for ax in axes:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "colors_tensor = torch.Tensor(classes[[\"r\", \"g\", \"b\"]].to_numpy()).to(int)\n",
        "\n",
        "for img_idx, iou in worst_imgs[:3]:\n",
        "    plot_error_map(val_data, model, img_idx, colors_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.transforms.functional as F\n",
        "\n",
        "class CamVidDatasetAug(CamVidDataset):\n",
        "    def __init__(self, type_, img_dim=[720, 960], dataset_path=None, device=\"cpu\", aug=True):\n",
        "        super().__init__(type_, img_dim, dataset_path, device)\n",
        "        self.use_aug = aug\n",
        "        \n",
        "        if self.use_aug:\n",
        "            self.tilt_prob = 0.5\n",
        "            self.flip_prob = 0.5\n",
        "            self.zoom_prob = 0.5\n",
        "            self.shear_prob = 0\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        def zoom(image, label, scale1, scale2=1.0):\n",
        "            i, j, h, w = T.RandomResizedCrop.get_params(\n",
        "                image, scale=(scale1, scale2), ratio=(1.0, 1.0)\n",
        "            )\n",
        "            image = F.resized_crop(image, i, j, h, w, image.shape[1:])\n",
        "            label = F.resized_crop(label, i, j, h, w, label.shape[1:], interpolation=F.InterpolationMode.NEAREST)\n",
        "            return image, label\n",
        "\n",
        "        # Pega o path da imagem e da label e carrega\n",
        "        img_path = self.data[idx]\n",
        "        name = os.path.basename(img_path).split('.')[0]\n",
        "        label_path = os.path.join(self.base_path_labels, f\"{name}_L.png\")\n",
        "\n",
        "        # Carrega como tensores na CPU\n",
        "        image = decode_image(img_path).to(torch.float)\n",
        "        label = decode_image(label_path)\n",
        "\n",
        "        # Aplica augmentation (se habilitado)\n",
        "        if self.use_aug:\n",
        "            if torch.rand(1) < self.tilt_prob:\n",
        "                angle = torch.randint(-10, 10, (1,)).item()\n",
        "                image = F.rotate(image, angle, fill=0)\n",
        "                label = F.rotate(label, angle, interpolation=F.InterpolationMode.NEAREST, fill=0)\n",
        "            \n",
        "            if torch.rand(1) < self.flip_prob:\n",
        "                image = F.hflip(image)\n",
        "                label = F.hflip(label)\n",
        "                \n",
        "            if torch.rand(1) < self.zoom_prob:\n",
        "                image, label = zoom(image, label, 0.5)\n",
        "\n",
        "                \n",
        "\n",
        "        # Aplica transformações de pré-processamento\n",
        "        image = self.resize_img(image)\n",
        "        image = self.normalize_img(image)\n",
        "        label = self.resize_label(label)\n",
        "\n",
        "        # Converte a máscara RGB para classes\n",
        "        label_permute = label.permute(1, 2, 0)\n",
        "        r_channel = label_permute[:, :, 0]\n",
        "        g_channel = label_permute[:, :, 1]\n",
        "        b_channel = label_permute[:, :, 2]\n",
        "        label = torch.from_numpy(self.lookup_classes[r_channel, g_channel, b_channel])\n",
        "\n",
        "        # Move os tensores finais para o device correto\n",
        "        return image.to(self.device), label.to(self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_hflip_augmentation(dataset_class, dataset_path, device, colors_tensor, image_index=0):\n",
        "    \"\"\"\n",
        "    Carrega a mesma imagem várias vezes de um dataset com augmentation\n",
        "    para verificar visualmente se a transformação (hflip) está sendo aplicada\n",
        "    corretamente tanto na imagem quanto na máscara.\n",
        "    \"\"\"\n",
        "    print(f\"Testando hflip para a imagem de índice {image_index}. Execute novamente para ver diferentes resultados aleatórios.\")\n",
        "    \n",
        "    # Instancia o dataset com augmentation ativada\n",
        "    aug_dataset = dataset_class(\n",
        "        \"train\", \n",
        "        img_dim=[256, 256], \n",
        "        dataset_path=[dataset_path], \n",
        "        device=device, \n",
        "        aug=True\n",
        "    )\n",
        "    \n",
        "    # Define os parâmetros de desnormalização para visualização\n",
        "    mean = torch.tensor([105.3549, 107.8312, 109.6686], device=device).view(3, 1, 1)\n",
        "    std = torch.tensor([5637.0488**0.5, 5869.9844**0.5, 5731.8906**0.5], device=device).view(3, 1, 1)\n",
        "    \n",
        "    # Plota a mesma imagem 4 vezes para observar o efeito aleatório do flip\n",
        "    fig, axes = plt.subplots(4, 2, figsize=(8, 16))\n",
        "    fig.suptitle(\"Verificação do Random Horizontal Flip (hflip)\", fontsize=16)\n",
        "\n",
        "    for i in range(4):\n",
        "        # Pega a imagem e a máscara do dataset\n",
        "        img_tensor, mask_tensor = aug_dataset[image_index]\n",
        "        \n",
        "        # 1. Prepara a imagem para visualização (desnormaliza)\n",
        "        img_display = img_tensor * std + mean\n",
        "        img_display = img_display.byte().cpu().permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # 2. Prepara a máscara para visualização (converte de classe para cor)\n",
        "        mask_display = colors_tensor[mask_tensor.long()].cpu().numpy().astype(np.uint8)\n",
        "        \n",
        "        # Plota a imagem\n",
        "        axes[i, 0].imshow(img_display)\n",
        "        axes[i, 0].set_title(f\"Tentativa {i+1}: Imagem\")\n",
        "        axes[i, 0].axis('off')\n",
        "        \n",
        "        # Plota a máscara\n",
        "        axes[i, 1].imshow(mask_display)\n",
        "        axes[i, 1].set_title(f\"Tentativa {i+1}: Máscara\")\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "# --- Célula para executar o teste ---\n",
        "# Certifique-se que as variáveis 'camvid_path', 'device' e 'classes' já foram definidas em células anteriores.\n",
        "colors_tensor = torch.Tensor(classes[[\"r\", \"g\", \"b\"]].to_numpy()).to(int)\n",
        "\n",
        "# Execute a função de teste\n",
        "test_hflip_augmentation(CamVidDatasetAug, camvid_path, device, colors_tensor, image_index=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot do histórico e comparações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_hist(hist, name = \"\"):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    fig.suptitle(f\"Histórico de treinamento - {name}\")\n",
        "\n",
        "    axes[0].plot(hist[\"train\"][\"loss\"], label = \"Train\")\n",
        "    axes[0].plot(hist[\"val\"][\"loss\"], label = \"Validation\")\n",
        "    axes[0].set_xlabel(\"Epoch\")\n",
        "    axes[0].set_ylabel(\"Loss de treino\")\n",
        "#    axes[0].set_title(\"Comparação de perda no treino\")\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot(hist[\"train\"][\"acc\"], label = \"Train\")\n",
        "    axes[1].plot(hist[\"val\"][\"acc\"], label = \"Validation\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Acurácia\")\n",
        "    axes[1].set_ylim((0, 1))\n",
        "#    axes[1].set_title(\"Comparação de acurácia no treino\")\n",
        "    axes[1].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_acc(hist_1, hist_2, name_1 = \"\", name_2 = \"\"):\n",
        "    plt.plot(hist_1[\"val\"][\"acc\"], label = name_1)\n",
        "    plt.plot(hist_2[\"val\"][\"acc\"], label = name_2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Acurácia\")\n",
        "    plt.ylim((0, 1))\n",
        "    plt.title(f\"Comparação de acurácias - {name_1} vs {name_2}\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepLab V3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeepLabV3(nn.Module):\n",
        "    def __init__(self, weights = DeepLabV3_ResNet50_Weights.DEFAULT):\n",
        "        super().__init__()\n",
        "        self.deeplab = deeplabv3_resnet50(weights = weights)\n",
        "\n",
        "        for param in self.deeplab.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.deeplab.classifier = DeepLabHead(2048, 32)\n",
        "        self.deeplab.aux_classifier = FCNHead(1024, 32)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.deeplab(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 635428,
          "sourceId": 1132317,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "studies",
      "language": "python",
      "name": "studies"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
