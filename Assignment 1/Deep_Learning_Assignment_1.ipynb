{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gOU8w75UkCl",
        "outputId": "189ce469-b618-4633-f2a2-e5d285beb026"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "camvid_path = kagglehub.dataset_download('carlolepelaars/camvid')\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dfvgCx6UkCn"
      },
      "source": [
        "\n",
        "# **Programming Assignment 1 - Semantic Segmentation**\n",
        "\n",
        "#### **Professor**: Dário Oliveira  \n",
        "#### **Monitor**: Lívia Meinhardt\n",
        "\n",
        "Neste trabalho prático, vocês irão investigar o desempenho de modelos de segmentação semântica, utilizando redes U-Net e DeepLabV3. A proposta vai além de treinar modelos: vocês deverão analisar onde eles funcionam bem (ou não), interpretar os erros e justificar suas decisões com base nos conceitos vistos em aula.\n",
        "\n",
        "\n",
        "### **Instruções:**\n",
        "\n",
        "1. **Escolha do Ambiente de Execução**:  \n",
        "   Utilize Google Colab ou Kaggle Notebook. Recomendamos iniciar seu notebook no Kaggle diretamente da página do dataset [CamVid](https://www.kaggle.com/datasets/carlolepelaars/camvid).\n",
        "\n",
        "3. **Criação de um Dataset Customizado**:  \n",
        "   As máscaras vêm em formato RGB. Use o CSV fornecido para converter as cores em labels (classe por pixel). Crie seu próprio Dataset em PyTorch.\n",
        "\n",
        "4. **Construção da Arquitetura U-Net**:  \n",
        "   Implemente uma U-Net (ou ResUNet), com a possibilidade de variar sua profundidade (número de blocos codificadores/decodificadores). Explore como isso impacta o desempenho e a quantidade de parâmetros.\n",
        "\n",
        "   ![Estrutura U-Net](https://camo.githubusercontent.com/6b548ee09b97874014d72903c891360beb0989e74b4585249436421558faa89d/68747470733a2f2f692e696d6775722e636f6d2f6a6544567071462e706e67)\n",
        "\n",
        "5. **Função de Treinamento**:  \n",
        "   Crie uma função de treinamento e registre métricas (loss, acurácia) em cada época. A cada 5 ou 10 épocas, visualize uma predição (imagem original, máscara verdadeira e predita).\n",
        "\n",
        "6. **Experimentação**:\n",
        "    Além da profundidade da rede e como ela afeta desempenho e quantidade de parâmetros, explore pelo menos dois otimizadores e funções de perda adequadas para segmentação semântica. Fundamente as escolhas e explique os resultados de acordo com a teoria vista em aula.\n",
        "\n",
        "8. **Avaliação**:\n",
        "   Implemente as métricas por classe: Precisão e IoU. Além da média geral, use para identificar as classes com pior desempenho e caracterizar seu melhor modelo.\n",
        "\n",
        "9. **Explicabilidade com Mapas de Erro**:\n",
        "   Escolha imagens com desempenho ruim (menor IoU ou precisão) e gere mapas de erro. Analise visualmente onde o modelo erra (bordas, classes confundidas, objetos pequenos, etc.). Relacione os erros às métricas.\n",
        "\n",
        "10. **Data Augmentation**:  \n",
        "     Implemente alguma forma de data augmentation. Avalie se houve ganho em desempenho. Justifique.\n",
        "\n",
        "11. **Fine-Tuning do DeepLabV3**:\n",
        "   Realize o fine-tuning do modelo [DeepLabV3](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.segmentation.deeplabv3_resnet50.html#torchvision.models.segmentation.deeplabv3_resnet50) pré-treinado ajustando `classifier` e `aux_classifier` para o número de classes do seu dataset. Congele o backbone conforme necessário e treine o modelo. Compare seu desempenho com a melhor U-Net em termos de métricas, número de parâmetros e qualidade visual das predições.\n",
        "\n",
        "12. **Apresentação Final**:  \n",
        "    Ao final, prepare uma apresentação resumindo os passos seguidos, resultados obtidos, gráficos de perdas e acurácia, e discussões sobre o desempenho do modelo. Lembre de fundamentar a discussão com os aspectos teoricos vistos em sala de aula.\n",
        "\n",
        "\n",
        "### **Pontos Importantes:**\n",
        "\n",
        "- Escolher adequadamente o tamanho do BATCH, Loss Function e Otimizador e saber o motivo de cada escolha;\n",
        "- Monitore o uso das GPUs, o kaggle te informa quantidade de tempo disponível, mas o colab não;\n",
        "- Observe as classes com mais erros por parte do modelo;\n",
        "- Adicione gráficos de perda e acurácia na sua apresentação;\n",
        "- Coloque imagens das predições do modelo;\n",
        "- Use **Pytorch !!!**.\n",
        "\n",
        "Note que as instruções acima são os requisitos da entrega, mas não precisam ser feitas exatamente nesta ordem. Você pode implementar o lógica de validação do modelo, testar uma versão inicial e modificar conforme os resultados obtidos. Lembre-se de fundamentas suas escolhas e fluxo de trabalho na apresentação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "execution": {
          "iopub.execute_input": "2025-09-07T17:47:54.065711Z",
          "iopub.status.busy": "2025-09-07T17:47:54.065426Z",
          "iopub.status.idle": "2025-09-07T17:48:06.838026Z",
          "shell.execute_reply": "2025-09-07T17:48:06.836932Z",
          "shell.execute_reply.started": "2025-09-07T17:47:54.065689Z"
        },
        "id": "GGfWsb5pUkCq",
        "outputId": "cd1cb65d-2b73-4cb6-9ddf-260b7e6839d4",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision.io import decode_image\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "f\"using device {device}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-09-07T17:48:06.839725Z",
          "iopub.status.busy": "2025-09-07T17:48:06.8393Z",
          "iopub.status.idle": "2025-09-07T17:48:08.909174Z",
          "shell.execute_reply": "2025-09-07T17:48:08.90834Z",
          "shell.execute_reply.started": "2025-09-07T17:48:06.839702Z"
        },
        "id": "F4Kc4KJQUkCr",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#----------------------------I/O no Kaggle------------------------------\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        continue\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-09-07T17:52:32.648449Z",
          "iopub.status.busy": "2025-09-07T17:52:32.64805Z",
          "iopub.status.idle": "2025-09-07T17:52:32.724985Z",
          "shell.execute_reply": "2025-09-07T17:52:32.724108Z",
          "shell.execute_reply.started": "2025-09-07T17:52:32.648422Z"
        },
        "id": "BAmPKkDGUkCs",
        "outputId": "a4f22134-91fe-4f41-92d4-4102dca8bbbf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#------------------Dataset para carregar as imagens---------------------\n",
        "class CamVidDataset(Dataset):\n",
        "\n",
        "    #inicializa o dataset\n",
        "    def __init__(self, type_, img_dim = [720, 960], dataset_path = (\"/\", \"kaggle\", \"input\", \"camvid\"), device = \"cpu\"):\n",
        "        #Carrega o caminho do conjunto desejado (val, train, test)\n",
        "        self.base_path_data = os.path.join(*dataset_path, \"CamVid\", f\"{type_}\")\n",
        "        self.base_path_labels = os.path.join(*dataset_path, \"CamVid\", f\"{type_}_labels\")\n",
        "        self.data = glob.glob(os.path.join(self.base_path_data, \"*.png\"))\n",
        "\n",
        "        #Carrega o csv com as classes e trata ele\n",
        "        df = pd.read_csv(os.path.join(*dataset_path, \"CamVid\", \"class_dict.csv\"))\n",
        "        df = df.reset_index().rename(columns = {\"index\": \"class\"})\n",
        "        self.df = df.set_index([\"r\", \"g\", \"b\"])\n",
        "\n",
        "        #Faz uma array de lookup pra converter o rgb pra classe mais eficientemente\n",
        "        self.lookup_classes = np.zeros((256, 256, 256), dtype = \"uint8\")\n",
        "        for (r, g, b), row in self.df.iterrows():\n",
        "            self.lookup_classes[r, g, b] = row[\"class\"]\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.resize_img = T.Resize(size=img_dim, interpolation=T.InterpolationMode.BILINEAR, antialias=True)\n",
        "        self.resize_label = T.Resize(size=img_dim, interpolation=T.InterpolationMode.NEAREST)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        #Pega o path da imagem e da label e carrega\n",
        "        img_path = self.data[idx]\n",
        "        name = img_path.split(\"/\")[-1].split(\".\")[0]\n",
        "        label_path = os.path.join(self.base_path_labels, f\"{name}_L.png\")\n",
        "\n",
        "        image = decode_image(img_path).to(torch.float).to(self.device)\n",
        "        label = decode_image(label_path)\n",
        "\n",
        "        image = self.resize_img(image)\n",
        "        label = self.resize_label(label)\n",
        "\n",
        "        #Procura na array de lookup as classes de cada pixel em um acesso só\n",
        "        #(mais eficiente)\n",
        "        label_permute = label.permute(1, 2, 0)\n",
        "        r_channel = label_permute[:, :, 0]\n",
        "        g_channel = label_permute[:, :, 1]\n",
        "        b_channel = label_permute[:, :, 2]\n",
        "        label = torch.Tensor(self.lookup_classes[r_channel, g_channel, b_channel]).to(self.device)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "train_data = CamVidDataset(\"train\", img_dim=[360, 480], dataset_path = [camvid_path], device = device)\n",
        "val_data = CamVidDataset(\"val\", img_dim=[360, 480], dataset_path = [camvid_path], device = device)\n",
        "test_data = CamVidDataset(\"test\", img_dim=[360, 480], dataset_path = [camvid_path], device = device)\n",
        "test_data[0][0].dtype, test_data[0][1].dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lf4Rm3kmdvrs",
        "outputId": "ed404dbd-b795-4081-95fb-c5424df6c53c"
      },
      "outputs": [],
      "source": [
        "#---------------Calculo a frequência das classes--------------------------\n",
        "classes = pd.read_csv(os.path.join(camvid_path, \"CamVid\", \"class_dict.csv\"))\n",
        "colors = torch.Tensor(classes[[\"r\", \"g\", \"b\"]].to_numpy()).to(int).to(device)\n",
        "\n",
        "freqs = torch.zeros((32,)).to(int).to(device)\n",
        "for image, label in train_data:\n",
        "    class_, count = label.unique(return_counts = True)\n",
        "    freqs[class_.to(int)] += count\n",
        "\n",
        "classes[\"frequencies\"] = freqs.cpu()\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl6vxavBUkCt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#---------------------------UNet incial----------------------------------\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv_block_3 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.conv_block_4 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.upsample_block_1 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            nn.Conv2d(in_channels = 512, out_channels = 1024, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 1024, out_channels = 1024, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels = 1024, out_channels = 512, kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.upsample_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 1024, out_channels = 512, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 512, out_channels = 512, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels = 512, out_channels = 256, kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.upsample_block_3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels = 256, out_channels = 128, kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.upsample_block_4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 256, out_channels = 128, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 128, out_channels = 128, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 2, stride = 2)\n",
        "        )\n",
        "\n",
        "        self.classifier_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 128, out_channels = 64, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, padding = \"same\"), nn.ReLU(),\n",
        "            nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 1, padding = \"same\"), nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        conv_1_out = self.conv_block_1(image)\n",
        "        conv_2_out = self.conv_block_2(conv_1_out)\n",
        "        conv_3_out = self.conv_block_3(conv_2_out)\n",
        "\n",
        "        conv_4_out = self.conv_block_4(conv_3_out)\n",
        "        up_1_out = self.upsample_block_1(conv_4_out)\n",
        "\n",
        "        cat_dim = len(up_1_out.shape) - 3\n",
        "\n",
        "        up_2_out = self.upsample_block_2(torch.cat((conv_4_out, up_1_out), dim = cat_dim))\n",
        "        up_3_out = self.upsample_block_3(torch.cat((conv_3_out, up_2_out), dim = cat_dim))\n",
        "        up_4_out = self.upsample_block_4(torch.cat((conv_2_out, up_3_out), dim = cat_dim))\n",
        "\n",
        "        logits = self.classifier_block(torch.cat((conv_1_out, up_4_out), dim = cat_dim))\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czzAxhDwdvrt",
        "outputId": "cbad83b1-76f9-4cb9-d48d-7807f8db98c7"
      },
      "outputs": [],
      "source": [
        "#-------------------------Inferência básica----------------------------------\n",
        "model = UNet().to(device)\n",
        "pred = model(test_data[0][0])\n",
        "pred_softmax = pred.argmax(axis = 0)\n",
        "print(pred.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "8R5e3Cspdvrt",
        "outputId": "7261bbe4-7541-4104-db01-e7f84d555b8c"
      },
      "outputs": [],
      "source": [
        "#---------------------Plot de imagens----------------------\n",
        "plt.imshow(colors[pred_softmax].cpu())\n",
        "#plt.imshow(test_data[0][0].permute(1, 2, 0).to(int))\n",
        "plt.imshow(colors[train_data[0][1].to(int)].to(int).cpu())\n",
        "train_data[0][1].unique(return_counts = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SydeFocKdvru"
      },
      "outputs": [],
      "source": [
        "#--------------------Loops de treino e teste-------------\n",
        "def loop_treino(dataloader, modelo, loss_fc, otimizador, epochs = 20):\n",
        "    tamanho = len(dataloader.dataset)\n",
        "    modelo.train()\n",
        "    for epoch in range(epochs):\n",
        "        loss_val = 0\n",
        "        for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "            # Calcula a previsão e a perda\n",
        "            pred = modelo(X)\n",
        "            loss = loss_fc(pred, y.to(int))\n",
        "\n",
        "            # Retropropagação\n",
        "            loss.backward()\n",
        "            otimizador.step()\n",
        "            otimizador.zero_grad()\n",
        "\n",
        "        loss_val += loss.item()\n",
        "        print(f\"Epoch {epoch}: {loss_val:>7f}\")\n",
        "\n",
        "\n",
        "def loop_teste(dataloader, modelo, loss_fc):\n",
        "    modelo.eval()\n",
        "\n",
        "    tamanho = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = modelo(X)\n",
        "            loss_test += loss_fc(pred, y).item()\n",
        "            acertos += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    loss_test /= num_batches\n",
        "    print(f\"Erro no Teste: \\n Perda média: {loss_test:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SEAdf0edvru"
      },
      "outputs": [],
      "source": [
        "#--------------Data loader, erro, otimizador, etc----------------------\n",
        "batch_size = 2\n",
        "train_dataloader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "val_dataloader = DataLoader(val_data, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr=0.001)\n",
        "\n",
        "weights = classes[\"frequencies\"].sum()/(classes[\"frequencies\"] + 1e0)\n",
        "error = nn.CrossEntropyLoss(weight = torch.Tensor(weights).to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNL1IPzJdvrv",
        "outputId": "f2edf5a6-7df7-447e-f53a-f2c409af6ba4"
      },
      "outputs": [],
      "source": [
        "loop_treino(train_dataloader, model, error, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKoYpx6kdvrv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 635428,
          "sourceId": 1132317,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "studies",
      "language": "python",
      "name": "studies"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
